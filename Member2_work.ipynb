{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d07b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LOADING ALL THREE DATASETS...\n",
      "\n",
      "================================================================================\n",
      "üìä LOADING: BORDERLINE\n",
      "üìÅ File: balanced_dataset_borderline_smote.csv\n",
      "================================================================================\n",
      "‚úÖ Successfully loaded!\n",
      "üìê Shape: (70000, 108) (70000 rows, 108 columns)\n",
      "\n",
      "üìã First 2 rows:\n",
      "   Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n",
      "0 -0.615849  -0.37982      -0.056439     -0.019356      0.001071   \n",
      "1  1.892744  -0.37982      -0.109265     -0.019759      0.001071   \n",
      "\n",
      "   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n",
      "0        -0.013169        -0.011306         0.598522        -0.267497   \n",
      "1        -0.010098        -0.010362         1.696897        -0.267497   \n",
      "\n",
      "   Fwd Pkt Len Mean  ...  latent_feature_12  latent_feature_13  \\\n",
      "0          0.432227  ...          -0.416912          -0.072620   \n",
      "1          1.555438  ...          -1.949246           0.060735   \n",
      "\n",
      "   latent_feature_14  latent_feature_15  latent_feature_16  latent_feature_17  \\\n",
      "0          -0.499757          -0.199525          -0.541627          -0.248089   \n",
      "1           4.092486          -0.758942           1.125454           0.995409   \n",
      "\n",
      "   latent_feature_18  latent_feature_19  latent_feature_20   Label  \n",
      "0          -0.213163          -0.083812          -0.103334  Benign  \n",
      "1           1.460620           1.708403          -0.049864  Benign  \n",
      "\n",
      "[2 rows x 108 columns]\n",
      "\n",
      "‚ùì No standard target columns found.\n",
      "All columns: ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Label']\n",
      "\n",
      "üìä Data types:\n",
      "float64    107\n",
      "object       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ No missing values\n",
      "\n",
      "üìà Numerical columns (107): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20']\n",
      "\n",
      "================================================================================\n",
      "üìä LOADING: ADASYN\n",
      "üìÅ File: balanced_dataset_adasyn.csv\n",
      "================================================================================\n",
      "‚úÖ Successfully loaded!\n",
      "üìê Shape: (70000, 108) (70000 rows, 108 columns)\n",
      "\n",
      "üìã First 2 rows:\n",
      "   Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n",
      "0 -0.615849  -0.37982      -0.056439     -0.019356      0.001071   \n",
      "1  1.892744  -0.37982      -0.109265     -0.019759      0.001071   \n",
      "\n",
      "   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n",
      "0        -0.013169        -0.011306         0.598522        -0.267497   \n",
      "1        -0.010098        -0.010362         1.696897        -0.267497   \n",
      "\n",
      "   Fwd Pkt Len Mean  ...  latent_feature_12  latent_feature_13  \\\n",
      "0          0.432227  ...          -0.416912          -0.072620   \n",
      "1          1.555438  ...          -1.949246           0.060735   \n",
      "\n",
      "   latent_feature_14  latent_feature_15  latent_feature_16  latent_feature_17  \\\n",
      "0          -0.499757          -0.199525          -0.541627          -0.248089   \n",
      "1           4.092486          -0.758942           1.125454           0.995409   \n",
      "\n",
      "   latent_feature_18  latent_feature_19  latent_feature_20   Label  \n",
      "0          -0.213163          -0.083812          -0.103334  Benign  \n",
      "1           1.460620           1.708403          -0.049864  Benign  \n",
      "\n",
      "[2 rows x 108 columns]\n",
      "\n",
      "‚ùì No standard target columns found.\n",
      "All columns: ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Label']\n",
      "\n",
      "üìä Data types:\n",
      "float64    107\n",
      "object       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ No missing values\n",
      "\n",
      "üìà Numerical columns (107): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20']\n",
      "\n",
      "================================================================================\n",
      "üìä LOADING: WEEK1\n",
      "üìÅ File: enhanced_dataset_week1.csv\n",
      "================================================================================\n",
      "‚úÖ Successfully loaded!\n",
      "üìê Shape: (100000, 130) (100000 rows, 130 columns)\n",
      "\n",
      "üìã First 2 rows:\n",
      "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
      "0 -0.635655 -1.373666  2018-02-16 01:00:32       4.549060     -0.020564   \n",
      "1 -0.634672 -0.379820  2018-02-16 01:01:42      -0.194848     -0.016135   \n",
      "\n",
      "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
      "0     -0.029437        -0.018644        -0.012130        -0.759993   \n",
      "1      0.062086        -0.006442         0.000192         1.195008   \n",
      "\n",
      "   Fwd Pkt Len Min  ...  Tot Bwd Pkts_rolling_min_100  \\\n",
      "0        -0.267497  ...                     -0.029437   \n",
      "1        -0.267497  ...                     -0.029437   \n",
      "\n",
      "   Tot Bwd Pkts_rolling_max_100  TotLen Fwd Pkts_rolling_mean_100  \\\n",
      "0                     -0.029437                         -0.018644   \n",
      "1                      0.062086                         -0.012543   \n",
      "\n",
      "   TotLen Fwd Pkts_rolling_std_100  TotLen Fwd Pkts_rolling_min_100  \\\n",
      "0                         0.008628                        -0.018644   \n",
      "1                         0.008628                        -0.018644   \n",
      "\n",
      "   TotLen Fwd Pkts_rolling_max_100  TotLen Bwd Pkts_rolling_mean_100  \\\n",
      "0                        -0.018644                         -0.012130   \n",
      "1                        -0.006442                         -0.005969   \n",
      "\n",
      "   TotLen Bwd Pkts_rolling_std_100  TotLen Bwd Pkts_rolling_min_100  \\\n",
      "0                         0.008713                         -0.01213   \n",
      "1                         0.008713                         -0.01213   \n",
      "\n",
      "   TotLen Bwd Pkts_rolling_max_100  \n",
      "0                        -0.012130  \n",
      "1                         0.000192  \n",
      "\n",
      "[2 rows x 130 columns]\n",
      "\n",
      "‚ùì No standard target columns found.\n",
      "All columns: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'source_file', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Flow Duration_rolling_mean_100', 'Flow Duration_rolling_std_100', 'Flow Duration_rolling_min_100', 'Flow Duration_rolling_max_100', 'Tot Fwd Pkts_rolling_mean_100', 'Tot Fwd Pkts_rolling_std_100', 'Tot Fwd Pkts_rolling_min_100', 'Tot Fwd Pkts_rolling_max_100', 'Tot Bwd Pkts_rolling_mean_100', 'Tot Bwd Pkts_rolling_std_100', 'Tot Bwd Pkts_rolling_min_100', 'Tot Bwd Pkts_rolling_max_100', 'TotLen Fwd Pkts_rolling_mean_100', 'TotLen Fwd Pkts_rolling_std_100', 'TotLen Fwd Pkts_rolling_min_100', 'TotLen Fwd Pkts_rolling_max_100', 'TotLen Bwd Pkts_rolling_mean_100', 'TotLen Bwd Pkts_rolling_std_100', 'TotLen Bwd Pkts_rolling_min_100', 'TotLen Bwd Pkts_rolling_max_100']\n",
      "\n",
      "üìä Data types:\n",
      "float64    127\n",
      "object       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ No missing values\n",
      "\n",
      "üìà Numerical columns (127): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Flow Duration_rolling_mean_100', 'Flow Duration_rolling_std_100', 'Flow Duration_rolling_min_100', 'Flow Duration_rolling_max_100', 'Tot Fwd Pkts_rolling_mean_100', 'Tot Fwd Pkts_rolling_std_100', 'Tot Fwd Pkts_rolling_min_100', 'Tot Fwd Pkts_rolling_max_100', 'Tot Bwd Pkts_rolling_mean_100', 'Tot Bwd Pkts_rolling_std_100', 'Tot Bwd Pkts_rolling_min_100', 'Tot Bwd Pkts_rolling_max_100', 'TotLen Fwd Pkts_rolling_mean_100', 'TotLen Fwd Pkts_rolling_std_100', 'TotLen Fwd Pkts_rolling_min_100', 'TotLen Fwd Pkts_rolling_max_100', 'TotLen Bwd Pkts_rolling_mean_100', 'TotLen Bwd Pkts_rolling_std_100', 'TotLen Bwd Pkts_rolling_min_100', 'TotLen Bwd Pkts_rolling_max_100']\n",
      "\n",
      "================================================================================\n",
      "üìã SUMMARY OF AVAILABLE DATASETS\n",
      "================================================================================\n",
      "üìÅ BORDERLINE | Shape: (70000, 108) | Columns: 108\n",
      "üìÅ ADASYN     | Shape: (70000, 108) | Columns: 108\n",
      "üìÅ WEEK1      | Shape: (100000, 130) | Columns: 130\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Correct file paths based on what we found\n",
    "correct_files = {\n",
    "    'borderline': 'balanced_dataset_borderline_smote.csv',  # Corrected name\n",
    "    'adasyn': 'balanced_dataset_adasyn.csv',                # Corrected name\n",
    "    'week1': 'enhanced_dataset_week1.csv'                   # This one was correct\n",
    "}\n",
    "\n",
    "downloads_path = Path.home() / \"Downloads\"\n",
    "\n",
    "def load_and_analyze_datasets():\n",
    "    \"\"\"Load and analyze all three datasets\"\"\"\n",
    "    datasets_info = {}\n",
    "    \n",
    "    for name, filename in correct_files.items():\n",
    "        file_path = downloads_path / filename\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä LOADING: {name.upper()}\")\n",
    "        print(f\"üìÅ File: {filename}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Store basic info\n",
    "            datasets_info[name] = {\n",
    "                'dataframe': df,\n",
    "                'shape': df.shape,\n",
    "                'columns': df.columns.tolist(),\n",
    "                'file_path': file_path\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded!\")\n",
    "            print(f\"üìê Shape: {df.shape} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "            \n",
    "            # Display first 2 rows to understand structure\n",
    "            print(f\"\\nüìã First 2 rows:\")\n",
    "            print(df.head(2))\n",
    "            \n",
    "            # Check for target column candidates\n",
    "            target_candidates = ['anomaly', 'label', 'class', 'target', 'is_anomaly', 'attack', 'malicious', 'Category']\n",
    "            found_targets = [col for col in target_candidates if col in df.columns]\n",
    "            \n",
    "            if found_targets:\n",
    "                print(f\"\\nüéØ Potential target columns: {found_targets}\")\n",
    "                for target_col in found_targets:\n",
    "                    print(f\"\\n--- Analysis of '{target_col}' ---\")\n",
    "                    print(f\"Unique values: {df[target_col].unique()}\")\n",
    "                    print(f\"Value counts:\")\n",
    "                    print(df[target_col].value_counts())\n",
    "                    \n",
    "                    # Calculate balance ratio\n",
    "                    value_counts = df[target_col].value_counts()\n",
    "                    balance_ratio = value_counts.min() / value_counts.max()\n",
    "                    print(f\"Balance ratio: {balance_ratio:.3f}\")\n",
    "                    \n",
    "                    if balance_ratio > 0.7:\n",
    "                        print(\"‚úÖ Highly balanced dataset\")\n",
    "                    elif balance_ratio > 0.3:\n",
    "                        print(\"‚úÖ Reasonably balanced\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è Imbalanced - may need special handling\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùì No standard target columns found.\")\n",
    "                print(f\"All columns: {df.columns.tolist()}\")\n",
    "                \n",
    "            # Data types and missing values\n",
    "            print(f\"\\nüìä Data types:\")\n",
    "            print(df.dtypes.value_counts())\n",
    "            \n",
    "            missing = df.isnull().sum()\n",
    "            if missing.sum() > 0:\n",
    "                print(f\"\\n‚ö†Ô∏è Missing values found:\")\n",
    "                print(missing[missing > 0])\n",
    "            else:\n",
    "                print(f\"\\n‚úÖ No missing values\")\n",
    "                \n",
    "            # Basic statistics for numerical columns\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numerical_cols) > 0:\n",
    "                print(f\"\\nüìà Numerical columns ({len(numerical_cols)}): {list(numerical_cols)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "    \n",
    "    return datasets_info\n",
    "\n",
    "# Load all three datasets\n",
    "print(\"üöÄ LOADING ALL THREE DATASETS...\")\n",
    "datasets_info = load_and_analyze_datasets()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìã SUMMARY OF AVAILABLE DATASETS\")\n",
    "print('='*80)\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"üìÅ {name.upper():<10} | Shape: {info['shape']} | Columns: {len(info['columns'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33dc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class ForensicTabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dim=128, depth=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.feature_embedding = nn.Linear(num_features, dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim, \n",
    "            nhead=heads, \n",
    "            dim_feedforward=dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_features]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Embed features\n",
    "        x = self.feature_embedding(x)  # [batch_size, dim]\n",
    "        \n",
    "        # Add sequence dimension for transformer\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, dim]\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)  # [batch_size, 1, dim]\n",
    "        \n",
    "        # Pool and classify\n",
    "        x = x.squeeze(1)  # [batch_size, dim]\n",
    "        x = self.classifier(x)  # [batch_size, num_classes]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ForensicTrainer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=1e-4, \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n",
    "        \n",
    "    def train_epoch(self, dataloader, criterion):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = output.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in dataloader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                preds = output.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, accuracy, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e67872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Choosing the best dataset for training...\n",
      "\n",
      "================================================================================\n",
      "üöÄ TRAINING FT-TRANSFORMER ON: BORDERLINE\n",
      "================================================================================\n",
      "üìä Dataset: borderline\n",
      "üìê Features: 107, Samples: 70000\n",
      "üéØ Classes: 7 (['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'Infilteration'])\n",
      "üìä Splits - Train: 44800, Val: 11200, Test: 14000\n",
      "\n",
      "üéØ Training started (Target: 96.0% accuracy)\n",
      "Epoch | Train Loss | Train Acc | Val Loss | Val Acc\n",
      "--------------------------------------------------\n",
      "    0 |    0.3507 |     0.86% |  0.1703 |   0.91%\n",
      "    1 |    0.1739 |     0.91% |  0.1632 |   0.91%\n",
      "    2 |    0.1625 |     0.91% |  0.1558 |   0.92%\n",
      "    3 |    0.1573 |     0.91% |  0.1536 |   0.92%\n",
      "    4 |    0.1536 |     0.92% |  0.1523 |   0.92%\n",
      "    5 |    0.1516 |     0.92% |  0.1493 |   0.92%\n",
      "    6 |    0.1501 |     0.92% |  0.1548 |   0.92%\n",
      "    7 |    0.1475 |     0.92% |  0.1481 |   0.92%\n",
      "    8 |    0.1467 |     0.92% |  0.1475 |   0.92%\n",
      "    9 |    0.1455 |     0.92% |  0.1459 |   0.92%\n",
      "   10 |    0.1438 |     0.92% |  0.1474 |   0.92%\n",
      "   15 |    0.1409 |     0.92% |  0.1447 |   0.92%\n",
      "   20 |    0.1378 |     0.92% |  0.1444 |   0.92%\n",
      "   25 |    0.1382 |     0.93% |  0.1484 |   0.92%\n",
      "\n",
      "üõë Early stopping at epoch 28\n",
      "\n",
      "üìä FINAL RESULTS:\n",
      "‚úÖ Best Validation Accuracy: 0.92%\n",
      "‚úÖ Test Accuracy: 0.93%\n",
      "\n",
      "üìà Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.87      0.56      0.68      2000\n",
      "                     Bot       1.00      1.00      1.00      2000\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00      2000\n",
      "    DDOS attack-LOIC-UDP       1.00      1.00      1.00      2000\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00      2000\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00      2000\n",
      "           Infilteration       0.68      0.92      0.78      2000\n",
      "\n",
      "                accuracy                           0.93     14000\n",
      "               macro avg       0.94      0.93      0.92     14000\n",
      "            weighted avg       0.94      0.93      0.92     14000\n",
      "\n",
      "\n",
      "‚ö†Ô∏è Accuracy 0.93% below target. Trying adasyn dataset...\n",
      "\n",
      "================================================================================\n",
      "üöÄ TRAINING FT-TRANSFORMER ON: ADASYN\n",
      "================================================================================\n",
      "üìä Dataset: adasyn\n",
      "üìê Features: 107, Samples: 70000\n",
      "üéØ Classes: 7 (['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'Infilteration'])\n",
      "üìä Splits - Train: 44800, Val: 11200, Test: 14000\n",
      "\n",
      "üéØ Training started (Target: 96.0% accuracy)\n",
      "Epoch | Train Loss | Train Acc | Val Loss | Val Acc\n",
      "--------------------------------------------------\n",
      "    0 |    0.3597 |     0.86% |  0.1711 |   0.91%\n",
      "    1 |    0.1733 |     0.91% |  0.1580 |   0.91%\n",
      "    2 |    0.1614 |     0.91% |  0.1541 |   0.92%\n",
      "    3 |    0.1573 |     0.91% |  0.1506 |   0.92%\n",
      "    4 |    0.1523 |     0.92% |  0.1504 |   0.92%\n",
      "    5 |    0.1501 |     0.92% |  0.1485 |   0.92%\n",
      "    6 |    0.1487 |     0.92% |  0.1486 |   0.92%\n",
      "    7 |    0.1471 |     0.92% |  0.1448 |   0.92%\n",
      "    8 |    0.1453 |     0.92% |  0.1447 |   0.92%\n",
      "    9 |    0.1455 |     0.92% |  0.1467 |   0.92%\n",
      "   10 |    0.1434 |     0.92% |  0.1456 |   0.92%\n",
      "   15 |    0.1398 |     0.92% |  0.1421 |   0.92%\n",
      "   20 |    0.1399 |     0.92% |  0.1433 |   0.92%\n",
      "   25 |    0.1387 |     0.92% |  0.1447 |   0.92%\n",
      "   30 |    0.1366 |     0.93% |  0.1432 |   0.92%\n",
      "\n",
      "üõë Early stopping at epoch 30\n",
      "\n",
      "üìä FINAL RESULTS:\n",
      "‚úÖ Best Validation Accuracy: 0.92%\n",
      "‚úÖ Test Accuracy: 0.92%\n",
      "\n",
      "üìà Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.88      0.54      0.67      2000\n",
      "                     Bot       1.00      1.00      1.00      2000\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00      2000\n",
      "    DDOS attack-LOIC-UDP       1.00      1.00      1.00      2000\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00      2000\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00      2000\n",
      "           Infilteration       0.67      0.92      0.77      2000\n",
      "\n",
      "                accuracy                           0.92     14000\n",
      "               macro avg       0.93      0.92      0.92     14000\n",
      "            weighted avg       0.93      0.92      0.92     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_and_train(dataset_name, datasets_info, target_accuracy=96.0):\n",
    "    \"\"\"Prepare data and train FT-Transformer\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ TRAINING FT-TRANSFORMER ON: {dataset_name.upper()}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    df = datasets_info[dataset_name]['dataframe']\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Label' in X:\n",
    "        X.remove('Label')\n",
    "    \n",
    "    features = df[X].values\n",
    "    labels = df['Label'].values\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    num_classes = len(le.classes_)\n",
    "    \n",
    "    print(f\"üìä Dataset: {dataset_name}\")\n",
    "    print(f\"üìê Features: {features.shape[1]}, Samples: {features.shape[0]}\")\n",
    "    print(f\"üéØ Classes: {num_classes} ({list(le.classes_)})\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_scaled, labels_encoded, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Splits - Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ForensicTabularDataset(X_train, y_train)\n",
    "    val_dataset = ForensicTabularDataset(X_val, y_val)\n",
    "    test_dataset = ForensicTabularDataset(X_test, y_test)\n",
    "    \n",
    "    # FIX: Set num_workers=0 for Windows compatibility\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "    class_weights = torch.FloatTensor(class_weights).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FTTransformer(\n",
    "        num_features=features.shape[1],\n",
    "        num_classes=num_classes,\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        heads=8\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ForensicTrainer(model)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nüéØ Training started (Target: {target_accuracy}% accuracy)\")\n",
    "    print(\"Epoch | Train Loss | Train Acc | Val Loss | Val Acc\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_loss, train_acc = trainer.train_epoch(train_loader, criterion)\n",
    "        val_loss, val_acc, _, _ = trainer.validate(val_loader, criterion)\n",
    "        trainer.scheduler.step()\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch < 10:\n",
    "            print(f\"{epoch:5d} | {train_loss:9.4f} | {train_acc:8.2f}% | {val_loss:7.4f} | {val_acc:6.2f}%\")\n",
    "        \n",
    "        # Early stopping and target achievement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_{dataset_name}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if val_acc >= target_accuracy:\n",
    "            print(f\"\\nüéâ TARGET ACHIEVED! Validation Accuracy: {val_acc:.2f}%\")\n",
    "            break\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nüõë Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model and test\n",
    "    model.load_state_dict(torch.load(f'best_model_{dataset_name}.pth'))\n",
    "    test_loss, test_acc, test_preds, test_targets = trainer.validate(test_loader, criterion)\n",
    "    \n",
    "    print(f\"\\nüìä FINAL RESULTS:\")\n",
    "    print(f\"‚úÖ Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"‚úÖ Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìà Classification Report:\")\n",
    "    print(classification_report(test_targets, test_preds, target_names=le.classes_))\n",
    "    \n",
    "    return model, scaler, le, test_acc\n",
    "\n",
    "# Add main guard for Windows compatibility\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç Choosing the best dataset for training...\")\n",
    "    \n",
    "    try:\n",
    "        model, scaler, le, test_acc = prepare_and_train('borderline', datasets_info, target_accuracy=96.0)\n",
    "        \n",
    "        if test_acc >= 96.0:\n",
    "            print(f\"\\nüéâ SUCCESS! Achieved {test_acc:.2f}% accuracy on borderline dataset!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Accuracy {test_acc:.2f}% below target. Trying adasyn dataset...\")\n",
    "            model, scaler, le, test_acc = prepare_and_train('adasyn', datasets_info, target_accuracy=96.0)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during training: {e}\")\n",
    "        print(\"Trying with smaller batch size...\")\n",
    "        \n",
    "        # Fallback with smaller batch size\n",
    "        try:\n",
    "            # You might need to modify the prepare_and_train function to accept batch_size as parameter\n",
    "            # or create a simplified version here\n",
    "            pass\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Final error: {e2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
