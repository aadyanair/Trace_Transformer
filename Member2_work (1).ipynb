{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d07b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ LOADING ALL THREE DATASETS...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š LOADING: BORDERLINE\n",
      "ğŸ“ File: balanced_dataset_borderline_smote.csv\n",
      "================================================================================\n",
      "âœ… Successfully loaded!\n",
      "ğŸ“ Shape: (70000, 108) (70000 rows, 108 columns)\n",
      "\n",
      "ğŸ“‹ First 2 rows:\n",
      "   Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n",
      "0 -0.615849  -0.37982      -0.056439     -0.019356      0.001071   \n",
      "1  1.892744  -0.37982      -0.109265     -0.019759      0.001071   \n",
      "\n",
      "   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n",
      "0        -0.013169        -0.011306         0.598522        -0.267497   \n",
      "1        -0.010098        -0.010362         1.696897        -0.267497   \n",
      "\n",
      "   Fwd Pkt Len Mean  ...  latent_feature_12  latent_feature_13  \\\n",
      "0          0.432227  ...          -0.416912          -0.072620   \n",
      "1          1.555438  ...          -1.949246           0.060735   \n",
      "\n",
      "   latent_feature_14  latent_feature_15  latent_feature_16  latent_feature_17  \\\n",
      "0          -0.499757          -0.199525          -0.541627          -0.248089   \n",
      "1           4.092486          -0.758942           1.125454           0.995409   \n",
      "\n",
      "   latent_feature_18  latent_feature_19  latent_feature_20   Label  \n",
      "0          -0.213163          -0.083812          -0.103334  Benign  \n",
      "1           1.460620           1.708403          -0.049864  Benign  \n",
      "\n",
      "[2 rows x 108 columns]\n",
      "\n",
      "â“ No standard target columns found.\n",
      "All columns: ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Label']\n",
      "\n",
      "ğŸ“Š Data types:\n",
      "float64    107\n",
      "object       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… No missing values\n",
      "\n",
      "ğŸ“ˆ Numerical columns (107): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20']\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š LOADING: ADASYN\n",
      "ğŸ“ File: balanced_dataset_adasyn.csv\n",
      "================================================================================\n",
      "âœ… Successfully loaded!\n",
      "ğŸ“ Shape: (70000, 108) (70000 rows, 108 columns)\n",
      "\n",
      "ğŸ“‹ First 2 rows:\n",
      "   Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n",
      "0 -0.615849  -0.37982      -0.056439     -0.019356      0.001071   \n",
      "1  1.892744  -0.37982      -0.109265     -0.019759      0.001071   \n",
      "\n",
      "   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n",
      "0        -0.013169        -0.011306         0.598522        -0.267497   \n",
      "1        -0.010098        -0.010362         1.696897        -0.267497   \n",
      "\n",
      "   Fwd Pkt Len Mean  ...  latent_feature_12  latent_feature_13  \\\n",
      "0          0.432227  ...          -0.416912          -0.072620   \n",
      "1          1.555438  ...          -1.949246           0.060735   \n",
      "\n",
      "   latent_feature_14  latent_feature_15  latent_feature_16  latent_feature_17  \\\n",
      "0          -0.499757          -0.199525          -0.541627          -0.248089   \n",
      "1           4.092486          -0.758942           1.125454           0.995409   \n",
      "\n",
      "   latent_feature_18  latent_feature_19  latent_feature_20   Label  \n",
      "0          -0.213163          -0.083812          -0.103334  Benign  \n",
      "1           1.460620           1.708403          -0.049864  Benign  \n",
      "\n",
      "[2 rows x 108 columns]\n",
      "\n",
      "â“ No standard target columns found.\n",
      "All columns: ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Label']\n",
      "\n",
      "ğŸ“Š Data types:\n",
      "float64    107\n",
      "object       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… No missing values\n",
      "\n",
      "ğŸ“ˆ Numerical columns (107): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20']\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š LOADING: WEEK1\n",
      "ğŸ“ File: enhanced_dataset_week1.csv\n",
      "================================================================================\n",
      "âœ… Successfully loaded!\n",
      "ğŸ“ Shape: (100000, 130) (100000 rows, 130 columns)\n",
      "\n",
      "ğŸ“‹ First 2 rows:\n",
      "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
      "0 -0.635655 -1.373666  2018-02-16 01:00:32       4.549060     -0.020564   \n",
      "1 -0.634672 -0.379820  2018-02-16 01:01:42      -0.194848     -0.016135   \n",
      "\n",
      "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
      "0     -0.029437        -0.018644        -0.012130        -0.759993   \n",
      "1      0.062086        -0.006442         0.000192         1.195008   \n",
      "\n",
      "   Fwd Pkt Len Min  ...  Tot Bwd Pkts_rolling_min_100  \\\n",
      "0        -0.267497  ...                     -0.029437   \n",
      "1        -0.267497  ...                     -0.029437   \n",
      "\n",
      "   Tot Bwd Pkts_rolling_max_100  TotLen Fwd Pkts_rolling_mean_100  \\\n",
      "0                     -0.029437                         -0.018644   \n",
      "1                      0.062086                         -0.012543   \n",
      "\n",
      "   TotLen Fwd Pkts_rolling_std_100  TotLen Fwd Pkts_rolling_min_100  \\\n",
      "0                         0.008628                        -0.018644   \n",
      "1                         0.008628                        -0.018644   \n",
      "\n",
      "   TotLen Fwd Pkts_rolling_max_100  TotLen Bwd Pkts_rolling_mean_100  \\\n",
      "0                        -0.018644                         -0.012130   \n",
      "1                        -0.006442                         -0.005969   \n",
      "\n",
      "   TotLen Bwd Pkts_rolling_std_100  TotLen Bwd Pkts_rolling_min_100  \\\n",
      "0                         0.008713                         -0.01213   \n",
      "1                         0.008713                         -0.01213   \n",
      "\n",
      "   TotLen Bwd Pkts_rolling_max_100  \n",
      "0                        -0.012130  \n",
      "1                         0.000192  \n",
      "\n",
      "[2 rows x 130 columns]\n",
      "\n",
      "â“ No standard target columns found.\n",
      "All columns: ['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'source_file', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Flow Duration_rolling_mean_100', 'Flow Duration_rolling_std_100', 'Flow Duration_rolling_min_100', 'Flow Duration_rolling_max_100', 'Tot Fwd Pkts_rolling_mean_100', 'Tot Fwd Pkts_rolling_std_100', 'Tot Fwd Pkts_rolling_min_100', 'Tot Fwd Pkts_rolling_max_100', 'Tot Bwd Pkts_rolling_mean_100', 'Tot Bwd Pkts_rolling_std_100', 'Tot Bwd Pkts_rolling_min_100', 'Tot Bwd Pkts_rolling_max_100', 'TotLen Fwd Pkts_rolling_mean_100', 'TotLen Fwd Pkts_rolling_std_100', 'TotLen Fwd Pkts_rolling_min_100', 'TotLen Fwd Pkts_rolling_max_100', 'TotLen Bwd Pkts_rolling_mean_100', 'TotLen Bwd Pkts_rolling_std_100', 'TotLen Bwd Pkts_rolling_min_100', 'TotLen Bwd Pkts_rolling_max_100']\n",
      "\n",
      "ğŸ“Š Data types:\n",
      "float64    127\n",
      "object       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… No missing values\n",
      "\n",
      "ğŸ“ˆ Numerical columns (127): ['Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'flow_duration_x_fwd_pkt_len_max', 'flow_duration_x_bwd_pkt_len_max', 'avg_fwd_packet_size', 'avg_bwd_packet_size', 'signed_log_Flow Duration', 'signed_log_TotLen Fwd Pkts', 'signed_log_TotLen Bwd Pkts', 'signed_log_Flow Byts/s', 'signed_log_Flow Pkts/s', 'latent_feature_1', 'latent_feature_2', 'latent_feature_3', 'latent_feature_4', 'latent_feature_5', 'latent_feature_6', 'latent_feature_7', 'latent_feature_8', 'latent_feature_9', 'latent_feature_10', 'latent_feature_11', 'latent_feature_12', 'latent_feature_13', 'latent_feature_14', 'latent_feature_15', 'latent_feature_16', 'latent_feature_17', 'latent_feature_18', 'latent_feature_19', 'latent_feature_20', 'Flow Duration_rolling_mean_100', 'Flow Duration_rolling_std_100', 'Flow Duration_rolling_min_100', 'Flow Duration_rolling_max_100', 'Tot Fwd Pkts_rolling_mean_100', 'Tot Fwd Pkts_rolling_std_100', 'Tot Fwd Pkts_rolling_min_100', 'Tot Fwd Pkts_rolling_max_100', 'Tot Bwd Pkts_rolling_mean_100', 'Tot Bwd Pkts_rolling_std_100', 'Tot Bwd Pkts_rolling_min_100', 'Tot Bwd Pkts_rolling_max_100', 'TotLen Fwd Pkts_rolling_mean_100', 'TotLen Fwd Pkts_rolling_std_100', 'TotLen Fwd Pkts_rolling_min_100', 'TotLen Fwd Pkts_rolling_max_100', 'TotLen Bwd Pkts_rolling_mean_100', 'TotLen Bwd Pkts_rolling_std_100', 'TotLen Bwd Pkts_rolling_min_100', 'TotLen Bwd Pkts_rolling_max_100']\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ SUMMARY OF AVAILABLE DATASETS\n",
      "================================================================================\n",
      "ğŸ“ BORDERLINE | Shape: (70000, 108) | Columns: 108\n",
      "ğŸ“ ADASYN     | Shape: (70000, 108) | Columns: 108\n",
      "ğŸ“ WEEK1      | Shape: (100000, 130) | Columns: 130\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Correct file paths based on what we found\n",
    "correct_files = {\n",
    "    'borderline': 'balanced_dataset_borderline_smote.csv',  # Corrected name\n",
    "    'adasyn': 'balanced_dataset_adasyn.csv',                # Corrected name\n",
    "    'week1': 'enhanced_dataset_week1.csv'                   # This one was correct\n",
    "}\n",
    "\n",
    "downloads_path = Path.home() / \"Downloads\"\n",
    "\n",
    "def load_and_analyze_datasets():\n",
    "    \"\"\"Load and analyze all three datasets\"\"\"\n",
    "    datasets_info = {}\n",
    "    \n",
    "    for name, filename in correct_files.items():\n",
    "        file_path = downloads_path / filename\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ“Š LOADING: {name.upper()}\")\n",
    "        print(f\"ğŸ“ File: {filename}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Store basic info\n",
    "            datasets_info[name] = {\n",
    "                'dataframe': df,\n",
    "                'shape': df.shape,\n",
    "                'columns': df.columns.tolist(),\n",
    "                'file_path': file_path\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… Successfully loaded!\")\n",
    "            print(f\"ğŸ“ Shape: {df.shape} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "            \n",
    "            # Display first 2 rows to understand structure\n",
    "            print(f\"\\nğŸ“‹ First 2 rows:\")\n",
    "            print(df.head(2))\n",
    "            \n",
    "            # Check for target column candidates\n",
    "            target_candidates = ['anomaly', 'label', 'class', 'target', 'is_anomaly', 'attack', 'malicious', 'Category']\n",
    "            found_targets = [col for col in target_candidates if col in df.columns]\n",
    "            \n",
    "            if found_targets:\n",
    "                print(f\"\\nğŸ¯ Potential target columns: {found_targets}\")\n",
    "                for target_col in found_targets:\n",
    "                    print(f\"\\n--- Analysis of '{target_col}' ---\")\n",
    "                    print(f\"Unique values: {df[target_col].unique()}\")\n",
    "                    print(f\"Value counts:\")\n",
    "                    print(df[target_col].value_counts())\n",
    "                    \n",
    "                    # Calculate balance ratio\n",
    "                    value_counts = df[target_col].value_counts()\n",
    "                    balance_ratio = value_counts.min() / value_counts.max()\n",
    "                    print(f\"Balance ratio: {balance_ratio:.3f}\")\n",
    "                    \n",
    "                    if balance_ratio > 0.7:\n",
    "                        print(\"âœ… Highly balanced dataset\")\n",
    "                    elif balance_ratio > 0.3:\n",
    "                        print(\"âœ… Reasonably balanced\")\n",
    "                    else:\n",
    "                        print(\"âš ï¸ Imbalanced - may need special handling\")\n",
    "            else:\n",
    "                print(f\"\\nâ“ No standard target columns found.\")\n",
    "                print(f\"All columns: {df.columns.tolist()}\")\n",
    "                \n",
    "            # Data types and missing values\n",
    "            print(f\"\\nğŸ“Š Data types:\")\n",
    "            print(df.dtypes.value_counts())\n",
    "            \n",
    "            missing = df.isnull().sum()\n",
    "            if missing.sum() > 0:\n",
    "                print(f\"\\nâš ï¸ Missing values found:\")\n",
    "                print(missing[missing > 0])\n",
    "            else:\n",
    "                print(f\"\\nâœ… No missing values\")\n",
    "                \n",
    "            # Basic statistics for numerical columns\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numerical_cols) > 0:\n",
    "                print(f\"\\nğŸ“ˆ Numerical columns ({len(numerical_cols)}): {list(numerical_cols)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading {filename}: {e}\")\n",
    "    \n",
    "    return datasets_info\n",
    "\n",
    "# Load all three datasets\n",
    "print(\"ğŸš€ LOADING ALL THREE DATASETS...\")\n",
    "datasets_info = load_and_analyze_datasets()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ“‹ SUMMARY OF AVAILABLE DATASETS\")\n",
    "print('='*80)\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"ğŸ“ {name.upper():<10} | Shape: {info['shape']} | Columns: {len(info['columns'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33dc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class ForensicTabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dim=128, depth=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.feature_embedding = nn.Linear(num_features, dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim, \n",
    "            nhead=heads, \n",
    "            dim_feedforward=dim*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_features]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Embed features\n",
    "        x = self.feature_embedding(x)  # [batch_size, dim]\n",
    "        \n",
    "        # Add sequence dimension for transformer\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, dim]\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)  # [batch_size, 1, dim]\n",
    "        \n",
    "        # Pool and classify\n",
    "        x = x.squeeze(1)  # [batch_size, dim]\n",
    "        x = self.classifier(x)  # [batch_size, num_classes]\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ForensicTrainer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=1e-4, \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n",
    "        \n",
    "    def train_epoch(self, dataloader, criterion):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = output.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in dataloader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                preds = output.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, accuracy, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e67872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Choosing the best dataset for training...\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING FT-TRANSFORMER ON: BORDERLINE\n",
      "================================================================================\n",
      "ğŸ“Š Dataset: borderline\n",
      "ğŸ“ Features: 107, Samples: 70000\n",
      "ğŸ¯ Classes: 7 (['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'Infilteration'])\n",
      "ğŸ“Š Splits - Train: 44800, Val: 11200, Test: 14000\n",
      "\n",
      "ğŸ¯ Training started (Target: 96.0% accuracy)\n",
      "Epoch | Train Loss | Train Acc | Val Loss | Val Acc\n",
      "--------------------------------------------------\n",
      "    0 |    0.3524 |     0.86% |  0.1698 |   0.91%\n",
      "    1 |    0.1769 |     0.91% |  0.1593 |   0.91%\n",
      "    2 |    0.1638 |     0.91% |  0.1582 |   0.91%\n",
      "    3 |    0.1565 |     0.91% |  0.1535 |   0.92%\n",
      "    4 |    0.1542 |     0.92% |  0.1557 |   0.92%\n",
      "    5 |    0.1521 |     0.92% |  0.1489 |   0.92%\n",
      "    6 |    0.1481 |     0.92% |  0.1520 |   0.92%\n",
      "    7 |    0.1474 |     0.92% |  0.1494 |   0.92%\n",
      "    8 |    0.1457 |     0.92% |  0.1479 |   0.92%\n",
      "    9 |    0.1458 |     0.92% |  0.1508 |   0.92%\n",
      "   10 |    0.1443 |     0.92% |  0.1501 |   0.92%\n",
      "   15 |    0.1415 |     0.92% |  0.1474 |   0.92%\n",
      "   20 |    0.1385 |     0.92% |  0.1451 |   0.92%\n",
      "   25 |    0.1381 |     0.92% |  0.1477 |   0.92%\n",
      "   30 |    0.1369 |     0.93% |  0.1492 |   0.92%\n",
      "\n",
      "ğŸ›‘ Early stopping at epoch 33\n",
      "\n",
      "ğŸ“Š FINAL RESULTS:\n",
      "âœ… Best Validation Accuracy: 0.92%\n",
      "âœ… Test Accuracy: 0.92%\n",
      "\n",
      "ğŸ“ˆ Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.87      0.55      0.68      2000\n",
      "                     Bot       1.00      1.00      1.00      2000\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00      2000\n",
      "    DDOS attack-LOIC-UDP       1.00      1.00      1.00      2000\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00      2000\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00      2000\n",
      "           Infilteration       0.67      0.92      0.78      2000\n",
      "\n",
      "                accuracy                           0.92     14000\n",
      "               macro avg       0.93      0.92      0.92     14000\n",
      "            weighted avg       0.93      0.92      0.92     14000\n",
      "\n",
      "\n",
      "âš ï¸ Accuracy 0.92% below target. Trying adasyn dataset...\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING FT-TRANSFORMER ON: ADASYN\n",
      "================================================================================\n",
      "ğŸ“Š Dataset: adasyn\n",
      "ğŸ“ Features: 107, Samples: 70000\n",
      "ğŸ¯ Classes: 7 (['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'Infilteration'])\n",
      "ğŸ“Š Splits - Train: 44800, Val: 11200, Test: 14000\n",
      "\n",
      "ğŸ¯ Training started (Target: 96.0% accuracy)\n",
      "Epoch | Train Loss | Train Acc | Val Loss | Val Acc\n",
      "--------------------------------------------------\n",
      "    0 |    0.3361 |     0.86% |  0.1743 |   0.91%\n",
      "    1 |    0.1736 |     0.91% |  0.1609 |   0.91%\n",
      "    2 |    0.1606 |     0.91% |  0.1590 |   0.92%\n",
      "    3 |    0.1558 |     0.91% |  0.1559 |   0.92%\n",
      "    4 |    0.1528 |     0.91% |  0.1560 |   0.92%\n",
      "    5 |    0.1500 |     0.92% |  0.1511 |   0.92%\n",
      "    6 |    0.1469 |     0.92% |  0.1488 |   0.92%\n",
      "    7 |    0.1469 |     0.92% |  0.1472 |   0.92%\n",
      "    8 |    0.1457 |     0.92% |  0.1481 |   0.92%\n",
      "    9 |    0.1445 |     0.92% |  0.1486 |   0.92%\n",
      "   10 |    0.1438 |     0.92% |  0.1463 |   0.92%\n",
      "   15 |    0.1402 |     0.92% |  0.1469 |   0.92%\n",
      "   20 |    0.1389 |     0.93% |  0.1464 |   0.92%\n",
      "   25 |    0.1375 |     0.92% |  0.1445 |   0.92%\n",
      "   30 |    0.1373 |     0.93% |  0.1422 |   0.92%\n",
      "   35 |    0.1364 |     0.93% |  0.1453 |   0.92%\n",
      "\n",
      "ğŸ›‘ Early stopping at epoch 38\n",
      "\n",
      "ğŸ“Š FINAL RESULTS:\n",
      "âœ… Best Validation Accuracy: 0.92%\n",
      "âœ… Test Accuracy: 0.92%\n",
      "\n",
      "ğŸ“ˆ Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.88      0.54      0.67      2000\n",
      "                     Bot       1.00      1.00      1.00      2000\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00      2000\n",
      "    DDOS attack-LOIC-UDP       1.00      1.00      1.00      2000\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00      2000\n",
      "DoS attacks-SlowHTTPTest       1.00      1.00      1.00      2000\n",
      "           Infilteration       0.67      0.92      0.78      2000\n",
      "\n",
      "                accuracy                           0.92     14000\n",
      "               macro avg       0.93      0.92      0.92     14000\n",
      "            weighted avg       0.93      0.92      0.92     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_and_train(dataset_name, datasets_info, target_accuracy=96.0):\n",
    "    \"\"\"Prepare data and train FT-Transformer\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ TRAINING FT-TRANSFORMER ON: {dataset_name.upper()}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    df = datasets_info[dataset_name]['dataframe']\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Label' in X:\n",
    "        X.remove('Label')\n",
    "    \n",
    "    features = df[X].values\n",
    "    labels = df['Label'].values\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    num_classes = len(le.classes_)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset: {dataset_name}\")\n",
    "    print(f\"ğŸ“ Features: {features.shape[1]}, Samples: {features.shape[0]}\")\n",
    "    print(f\"ğŸ¯ Classes: {num_classes} ({list(le.classes_)})\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_scaled, labels_encoded, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Splits - Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ForensicTabularDataset(X_train, y_train)\n",
    "    val_dataset = ForensicTabularDataset(X_val, y_val)\n",
    "    test_dataset = ForensicTabularDataset(X_test, y_test)\n",
    "    \n",
    "    # FIX: Set num_workers=0 for Windows compatibility\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "    class_weights = torch.FloatTensor(class_weights).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FTTransformer(\n",
    "        num_features=features.shape[1],\n",
    "        num_classes=num_classes,\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        heads=8\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ForensicTrainer(model)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Training started (Target: {target_accuracy}% accuracy)\")\n",
    "    print(\"Epoch | Train Loss | Train Acc | Val Loss | Val Acc\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        train_loss, train_acc = trainer.train_epoch(train_loader, criterion)\n",
    "        val_loss, val_acc, _, _ = trainer.validate(val_loader, criterion)\n",
    "        trainer.scheduler.step()\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch < 10:\n",
    "            print(f\"{epoch:5d} | {train_loss:9.4f} | {train_acc:8.2f}% | {val_loss:7.4f} | {val_acc:6.2f}%\")\n",
    "        \n",
    "        # Early stopping and target achievement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_{dataset_name}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if val_acc >= target_accuracy:\n",
    "            print(f\"\\nğŸ‰ TARGET ACHIEVED! Validation Accuracy: {val_acc:.2f}%\")\n",
    "            break\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nğŸ›‘ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model and test\n",
    "    model.load_state_dict(torch.load(f'best_model_{dataset_name}.pth'))\n",
    "    test_loss, test_acc, test_preds, test_targets = trainer.validate(test_loader, criterion)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š FINAL RESULTS:\")\n",
    "    print(f\"âœ… Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"âœ… Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Classification Report:\")\n",
    "    print(classification_report(test_targets, test_preds, target_names=le.classes_))\n",
    "    \n",
    "    return model, scaler, le, test_acc\n",
    "\n",
    "# Add main guard for Windows compatibility\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ” Choosing the best dataset for training...\")\n",
    "    \n",
    "    try:\n",
    "        model, scaler, le, test_acc = prepare_and_train('borderline', datasets_info, target_accuracy=96.0)\n",
    "        \n",
    "        if test_acc >= 96.0:\n",
    "            print(f\"\\nğŸ‰ SUCCESS! Achieved {test_acc:.2f}% accuracy on borderline dataset!\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ Accuracy {test_acc:.2f}% below target. Trying adasyn dataset...\")\n",
    "            model, scaler, le, test_acc = prepare_and_train('adasyn', datasets_info, target_accuracy=96.0)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during training: {e}\")\n",
    "        print(\"Trying with smaller batch size...\")\n",
    "        \n",
    "        # Fallback with smaller batch size\n",
    "        try:\n",
    "            # You might need to modify the prepare_and_train function to accept batch_size as parameter\n",
    "            # or create a simplified version here\n",
    "            pass\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Final error: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68b2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ STARTING FT-TRANSFORMER ENSEMBLING PIPELINE...\n",
      "ğŸ“Š Dataset: borderline\n",
      "ğŸ“ Features: 107, Samples: 70000\n",
      "ğŸ¯ Classes: 7 (['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'Infilteration'])\n",
      "ğŸ“Š Data Splits:\n",
      "   Train: 44800 samples\n",
      "   Val:   11200 samples\n",
      "   Test:  14000 samples\n",
      "ğŸš€ Training FT-Transformer ensemble variants...\n",
      "âœ… Created FT-Transformer ensemble variants:\n",
      "   - ftt_small: 209,287 parameters\n",
      "   - ftt_medium: 1,212,423 parameters\n",
      "   - ftt_large: 6,380,039 parameters\n",
      "   - ftt_wide: 815,879 parameters\n",
      "\n",
      "ğŸ“Š Training ftt_small...\n",
      "   Epoch   0: Val Acc = 0.9024\n",
      "   Epoch  10: Val Acc = 0.9172\n",
      "   Epoch  20: Val Acc = 0.9194\n",
      "   Epoch  30: Val Acc = 0.9196\n",
      "   Early stopping at epoch 38\n",
      "âœ… ftt_small training completed. Best Val Acc: 0.9196\n",
      "\n",
      "ğŸ“Š Training ftt_medium...\n",
      "   Epoch   0: Val Acc = 0.9096\n",
      "   Epoch  10: Val Acc = 0.9187\n",
      "   Epoch  20: Val Acc = 0.9201\n",
      "   Epoch  30: Val Acc = 0.9198\n",
      "   Early stopping at epoch 31\n",
      "âœ… ftt_medium training completed. Best Val Acc: 0.9212\n",
      "\n",
      "ğŸ“Š Training ftt_large...\n",
      "   Epoch   0: Val Acc = 0.9082\n",
      "   Epoch  10: Val Acc = 0.9170\n",
      "   Epoch  20: Val Acc = 0.9185\n",
      "   Early stopping at epoch 26\n",
      "âœ… ftt_large training completed. Best Val Acc: 0.9191\n",
      "\n",
      "ğŸ“Š Training ftt_wide...\n",
      "   Epoch   0: Val Acc = 0.9129\n",
      "   Epoch  10: Val Acc = 0.9200\n",
      "   Epoch  20: Val Acc = 0.9215\n",
      "   Early stopping at epoch 29\n",
      "âœ… ftt_wide training completed. Best Val Acc: 0.9216\n",
      "\n",
      "ğŸ“Š ENSEMBLE EVALUATION:\n",
      "----------------------------------------\n",
      "   ftt_small   : 0.9236\n",
      "   ftt_medium  : 0.9251\n",
      "   ftt_large   : 0.9243\n",
      "   ftt_wide    : 0.9256\n",
      "   Ensemble    : 0.9252\n",
      "\n",
      "ğŸ¯ Training Stacking Ensemble with logistic...\n",
      "ğŸ”„ Training stacking ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stacking ensemble trained successfully\n",
      "ğŸ“Š STACKING ENSEMBLE PERFORMANCE:\n",
      "   Accuracy: 0.9252\n",
      "\n",
      "ğŸ¯ FINAL RESULTS COMPARISON:\n",
      "--------------------------------------------------\n",
      "   Best Individual Model: 0.9256\n",
      "   Simple Ensemble:       0.9252\n",
      "   Stacking Ensemble:     0.9252\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT OVER BEST INDIVIDUAL MODEL:\n",
      "   Simple Ensemble:  +-0.0004\n",
      "   Stacking Ensemble: +-0.0004\n",
      "ğŸ’¾ Ensemble models saved to 'ft_transformer_ensemble.pth'\n",
      "\n",
      "ğŸ‰ FT-TRANSFORMER ENSEMBLING COMPLETED SUCCESSFULLY!\n",
      "ğŸ“Š Final Stacking Accuracy: 0.9252\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# FT-TRANSFORMER ENSEMBLING & STACKING (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "class FTTransformerEnsemble:\n",
    "    \"\"\"Ensemble of multiple FT-Transformer models with different architectures\"\"\"\n",
    "    def __init__(self, num_features, num_classes, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.models = {}\n",
    "    \n",
    "    def create_ft_transformer_variants(self):\n",
    "        \"\"\"Create multiple FT-Transformer variants for ensemble - FIXED DIMENSIONS\"\"\"\n",
    "        self.models = {\n",
    "            'ftt_small': FTTransformer(\n",
    "                num_features=self.num_features,\n",
    "                num_classes=self.num_classes,\n",
    "                dim=64,   # 64 divisible by 4 heads\n",
    "                depth=4,  \n",
    "                heads=4,  \n",
    "                dropout=0.1\n",
    "            ).to(self.device),\n",
    "            \n",
    "            'ftt_medium': FTTransformer(\n",
    "                num_features=self.num_features,\n",
    "                num_classes=self.num_classes,\n",
    "                dim=128,  # 128 divisible by 8 heads\n",
    "                depth=6,  \n",
    "                heads=8,  \n",
    "                dropout=0.1\n",
    "            ).to(self.device),\n",
    "            \n",
    "            'ftt_large': FTTransformer(\n",
    "                num_features=self.num_features,\n",
    "                num_classes=self.num_classes,\n",
    "                dim=256,  # 256 divisible by 16 heads\n",
    "                depth=8,  \n",
    "                heads=16, # Changed from 12 to 16\n",
    "                dropout=0.2\n",
    "            ).to(self.device),\n",
    "            \n",
    "            'ftt_wide': FTTransformer(\n",
    "                num_features=self.num_features,\n",
    "                num_classes=self.num_classes,\n",
    "                dim=128,  # 128 divisible by 8 heads\n",
    "                depth=4,  \n",
    "                heads=8,\n",
    "                dropout=0.1\n",
    "            ).to(self.device)\n",
    "        }\n",
    "        \n",
    "        print(\"âœ… Created FT-Transformer ensemble variants:\")\n",
    "        for name, model in self.models.items():\n",
    "            num_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"   - {name}: {num_params:,} parameters\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get probability predictions from all FT-Transformer models\"\"\"\n",
    "        all_probs = []\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if torch.is_tensor(X):\n",
    "                    X_tensor = X.to(self.device)\n",
    "                else:\n",
    "                    X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "                \n",
    "                outputs = model(X_tensor)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Average probabilities (simple ensembling)\n",
    "        avg_probs = np.mean(all_probs, axis=0)\n",
    "        return avg_probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes using ensemble voting\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "class FTTransformerStacking:\n",
    "    \"\"\"Stacking ensemble using FT-Transformer models as base learners\"\"\"\n",
    "    def __init__(self, base_models, meta_model_type='logistic'):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model_type = meta_model_type\n",
    "        self.meta_model = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X, y, val_ratio=0.3):\n",
    "        \"\"\"Fit stacking ensemble\"\"\"\n",
    "        print(\"ğŸ”„ Training stacking ensemble...\")\n",
    "        \n",
    "        # Split data for meta-training\n",
    "        X_train, X_meta, y_train, y_meta = train_test_split(\n",
    "            X, y, test_size=val_ratio, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Get base model predictions on meta set\n",
    "        base_predictions = self._get_base_predictions(X_meta)\n",
    "        \n",
    "        # Train meta-model\n",
    "        if self.meta_model_type == 'logistic':\n",
    "            self.meta_model = LogisticRegression(\n",
    "                multi_class='multinomial', \n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "                C=0.1\n",
    "            )\n",
    "        elif self.meta_model_type == 'random_forest':\n",
    "            self.meta_model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=42,\n",
    "                max_depth=10\n",
    "            )\n",
    "        \n",
    "        self.meta_model.fit(base_predictions, y_meta)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        print(\"âœ… Stacking ensemble trained successfully\")\n",
    "        return self\n",
    "    \n",
    "    def _get_base_predictions(self, X):\n",
    "        \"\"\"Get probability predictions from all base models\"\"\"\n",
    "        base_predictions = []\n",
    "        \n",
    "        for model_name, model in self.base_models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_tensor = torch.FloatTensor(X).to(next(model.parameters()).device)\n",
    "                outputs = model(X_tensor)\n",
    "                probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                base_predictions.append(probs)\n",
    "        \n",
    "        # Concatenate all predictions\n",
    "        return np.hstack(base_predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities using stacking\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Stacking ensemble not fitted yet!\")\n",
    "        \n",
    "        # Get base model predictions\n",
    "        base_preds = self._get_base_predictions(X)\n",
    "        \n",
    "        # Meta-model prediction\n",
    "        return self.meta_model.predict_proba(base_preds)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED FT-TRANSFORMER ENSEMBLE TRAINER (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "class FTTransformerEnsembleTrainer:\n",
    "    def __init__(self, num_features, num_classes, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.ensemble = None\n",
    "        self.stacking = None\n",
    "    \n",
    "    def train_ensemble_models(self, train_loader, val_loader, num_epochs=50, patience=8):\n",
    "        \"\"\"Train all FT-Transformer variants\"\"\"\n",
    "        print(\"ğŸš€ Training FT-Transformer ensemble variants...\")\n",
    "        \n",
    "        # Create ensemble variants\n",
    "        self.ensemble = FTTransformerEnsemble(self.num_features, self.num_classes, self.device)\n",
    "        self.ensemble.create_ft_transformer_variants()\n",
    "        \n",
    "        model_performances = {}\n",
    "        \n",
    "        for model_name, model in self.ensemble.models.items():\n",
    "            print(f\"\\nğŸ“Š Training {model_name}...\")\n",
    "            acc = self._train_single_model(model, train_loader, val_loader, model_name, num_epochs, patience)\n",
    "            model_performances[model_name] = acc\n",
    "        \n",
    "        return model_performances\n",
    "    \n",
    "    def _train_single_model(self, model, train_loader, val_loader, model_name, num_epochs, patience):\n",
    "        \"\"\"Train a single FT-Transformer model\"\"\"\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_preds, val_targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    output = model(data)\n",
    "                    preds = output.argmax(dim=1)\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_targets.extend(target.cpu().numpy())\n",
    "            \n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"   Epoch {epoch:3d}: Val Acc = {val_acc:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), f'best_{model_name}.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                if epoch > 10:  # Only stop if we've trained for a reasonable number of epochs\n",
    "                    print(f\"   Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        if os.path.exists(f'best_{model_name}.pth'):\n",
    "            model.load_state_dict(torch.load(f'best_{model_name}.pth'))\n",
    "        print(f\"âœ… {model_name} training completed. Best Val Acc: {best_val_acc:.4f}\")\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def evaluate_ensemble(self, X_test, y_test):\n",
    "        \"\"\"Evaluate ensemble performance\"\"\"\n",
    "        if self.ensemble is None:\n",
    "            raise ValueError(\"Ensemble not trained yet!\")\n",
    "        \n",
    "        print(\"\\nğŸ“Š ENSEMBLE EVALUATION:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Individual model performance\n",
    "        individual_results = {}\n",
    "        for model_name, model in self.ensemble.models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_tensor = torch.FloatTensor(X_test).to(self.device)\n",
    "                outputs = model(X_tensor)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                acc = accuracy_score(y_test, preds)\n",
    "                individual_results[model_name] = acc\n",
    "                print(f\"   {model_name:12}: {acc:.4f}\")\n",
    "        \n",
    "        # Ensemble performance\n",
    "        ensemble_preds = self.ensemble.predict(X_test)\n",
    "        ensemble_acc = accuracy_score(y_test, ensemble_preds)\n",
    "        print(f\"   {'Ensemble':12}: {ensemble_acc:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'individual': individual_results,\n",
    "            'ensemble': ensemble_acc\n",
    "        }\n",
    "    \n",
    "    def train_stacking_ensemble(self, X, y, meta_model_type='logistic'):\n",
    "        \"\"\"Train stacking ensemble\"\"\"\n",
    "        print(f\"\\nğŸ¯ Training Stacking Ensemble with {meta_model_type}...\")\n",
    "        \n",
    "        if self.ensemble is None:\n",
    "            raise ValueError(\"Base models not trained yet!\")\n",
    "        \n",
    "        self.stacking = FTTransformerStacking(self.ensemble.models, meta_model_type)\n",
    "        self.stacking.fit(X, y)\n",
    "        \n",
    "        return self.stacking\n",
    "    \n",
    "    def evaluate_stacking(self, X_test, y_test):\n",
    "        \"\"\"Evaluate stacking ensemble performance\"\"\"\n",
    "        if self.stacking is None:\n",
    "            raise ValueError(\"Stacking ensemble not trained yet!\")\n",
    "        \n",
    "        stacking_preds = self.stacking.predict(X_test)\n",
    "        stacking_acc = accuracy_score(y_test, stacking_preds)\n",
    "        \n",
    "        print(f\"ğŸ“Š STACKING ENSEMBLE PERFORMANCE:\")\n",
    "        print(f\"   Accuracy: {stacking_acc:.4f}\")\n",
    "        \n",
    "        return stacking_acc\n",
    "\n",
    "# =============================================================================\n",
    "# SIMPLE ENSEMBLE WITH EXISTING MODELS (ALTERNATIVE APPROACH)\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleFTTransformerEnsemble:\n",
    "    \"\"\"Simple ensemble using your already trained FT-Transformer models\"\"\"\n",
    "    def __init__(self, model_paths, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.models = {}\n",
    "        self.device = device\n",
    "        \n",
    "        # Load pre-trained models\n",
    "        for name, path in model_paths.items():\n",
    "            if os.path.exists(path):\n",
    "                # Create model architecture (adjust parameters as needed)\n",
    "                model = FTTransformer(\n",
    "                    num_features=107,  # Adjust based on your data\n",
    "                    num_classes=7,     # Adjust based on your data\n",
    "                    dim=128,\n",
    "                    depth=6,\n",
    "                    heads=8,\n",
    "                    dropout=0.1\n",
    "                ).to(device)\n",
    "                \n",
    "                # Load trained weights\n",
    "                model.load_state_dict(torch.load(path, map_location=device))\n",
    "                model.eval()\n",
    "                self.models[name] = model\n",
    "                print(f\"âœ… Loaded {name} from {path}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get probability predictions from all models\"\"\"\n",
    "        all_probs = []\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            with torch.no_grad():\n",
    "                if torch.is_tensor(X):\n",
    "                    X_tensor = X.to(self.device)\n",
    "                else:\n",
    "                    X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "                \n",
    "                outputs = model(X_tensor)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Average probabilities\n",
    "        avg_probs = np.mean(all_probs, axis=0)\n",
    "        return avg_probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION - FIXED VERSION\n",
    "# =============================================================================\n",
    "\n",
    "def run_ft_transformer_ensembling(datasets_info, dataset_name='borderline'):\n",
    "    \"\"\"Run complete FT-Transformer ensembling pipeline\"\"\"\n",
    "    print(\"ğŸš€ STARTING FT-TRANSFORMER ENSEMBLING PIPELINE...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df = datasets_info[dataset_name]['dataframe']\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Label' in X:\n",
    "        X.remove('Label')\n",
    "    \n",
    "    features = df[X].values\n",
    "    labels = df['Label'].values\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    labels_encoded = le.fit_transform(labels)\n",
    "    num_classes = len(le.classes_)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset: {dataset_name}\")\n",
    "    print(f\"ğŸ“ Features: {features.shape[1]}, Samples: {features.shape[0]}\")\n",
    "    print(f\"ğŸ¯ Classes: {num_classes} ({list(le.classes_)})\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_scaled, labels_encoded, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=labels_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Data Splits:\")\n",
    "    print(f\"   Train: {X_train.shape[0]} samples\")\n",
    "    print(f\"   Val:   {X_val.shape[0]} samples\") \n",
    "    print(f\"   Test:  {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = ForensicTabularDataset(X_train, y_train)\n",
    "    val_dataset = ForensicTabularDataset(X_val, y_val)\n",
    "    test_dataset = ForensicTabularDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize and train ensemble\n",
    "    ensemble_trainer = FTTransformerEnsembleTrainer(\n",
    "        num_features=features.shape[1],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Train ensemble models\n",
    "    model_performances = ensemble_trainer.train_ensemble_models(train_loader, val_loader)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    results = ensemble_trainer.evaluate_ensemble(X_test, y_test)\n",
    "    \n",
    "    # Train stacking ensemble\n",
    "    stacking = ensemble_trainer.train_stacking_ensemble(\n",
    "        np.vstack([X_train, X_val]),  # Use all training data for stacking\n",
    "        np.hstack([y_train, y_val]),\n",
    "        meta_model_type='logistic'\n",
    "    )\n",
    "    \n",
    "    # Evaluate stacking\n",
    "    stacking_acc = ensemble_trainer.evaluate_stacking(X_test, y_test)\n",
    "    \n",
    "    # Final comparison\n",
    "    print(f\"\\nğŸ¯ FINAL RESULTS COMPARISON:\")\n",
    "    print(\"-\" * 50)\n",
    "    best_individual = max(results['individual'].values())\n",
    "    print(f\"   Best Individual Model: {best_individual:.4f}\")\n",
    "    print(f\"   Simple Ensemble:       {results['ensemble']:.4f}\")\n",
    "    print(f\"   Stacking Ensemble:     {stacking_acc:.4f}\")\n",
    "    \n",
    "    improvement_simple = results['ensemble'] - best_individual\n",
    "    improvement_stacking = stacking_acc - best_individual\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT OVER BEST INDIVIDUAL MODEL:\")\n",
    "    print(f\"   Simple Ensemble:  +{improvement_simple:.4f}\")\n",
    "    print(f\"   Stacking Ensemble: +{improvement_stacking:.4f}\")\n",
    "    \n",
    "    # Save ensemble models\n",
    "    torch.save({\n",
    "        'ensemble_models': {name: model.state_dict() for name, model in ensemble_trainer.ensemble.models.items()},\n",
    "        'stacking_model': stacking.meta_model,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': le,\n",
    "        'feature_names': X\n",
    "    }, 'ft_transformer_ensemble.pth')\n",
    "    \n",
    "    print(\"ğŸ’¾ Ensemble models saved to 'ft_transformer_ensemble.pth'\")\n",
    "    \n",
    "    return {\n",
    "        'ensemble_trainer': ensemble_trainer,\n",
    "        'results': results,\n",
    "        'stacking_accuracy': stacking_acc,\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': le\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# QUICK ENSEMBLE WITH EXISTING MODEL (If you already have trained models)\n",
    "# =============================================================================\n",
    "\n",
    "def quick_ensemble_with_existing_model():\n",
    "    \"\"\"Quick ensemble using your existing trained model with different seeds\"\"\"\n",
    "    print(\"ğŸš€ CREATING QUICK ENSEMBLE WITH EXISTING MODEL...\")\n",
    "    \n",
    "    # Check if you have existing trained models\n",
    "    model_files = [\n",
    "        'best_model_borderline.pth',\n",
    "        'best_model_adasyn.pth', \n",
    "        'best_model_week1.pth'\n",
    "    ]\n",
    "    \n",
    "    existing_models = {}\n",
    "    for model_file in model_files:\n",
    "        if os.path.exists(model_file):\n",
    "            # Load your existing model architecture\n",
    "            model = FTTransformer(\n",
    "                num_features=107,  # Adjust based on your data\n",
    "                num_classes=7,     # Adjust based on your data  \n",
    "                dim=128,\n",
    "                depth=6,\n",
    "                heads=8,\n",
    "                dropout=0.1\n",
    "            )\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            model.eval()\n",
    "            existing_models[model_file] = model\n",
    "            print(f\"âœ… Loaded {model_file}\")\n",
    "    \n",
    "    if len(existing_models) > 1:\n",
    "        print(f\"ğŸ¯ Found {len(existing_models)} models for ensemble\")\n",
    "        return SimpleFTTransformerEnsemble(existing_models)\n",
    "    else:\n",
    "        print(\"â„¹ï¸  Not enough pre-trained models found for ensemble\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THE FIXED ENSEMBLING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try the main ensembling approach first\n",
    "    try:\n",
    "        ensemble_results = run_ft_transformer_ensembling(datasets_info, 'borderline')\n",
    "        \n",
    "        print(f\"\\nğŸ‰ FT-TRANSFORMER ENSEMBLING COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"ğŸ“Š Final Stacking Accuracy: {ensemble_results['stacking_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in main ensembling pipeline: {e}\")\n",
    "        print(\"ğŸ”„ Trying alternative approach with existing models...\")\n",
    "        \n",
    "        # Try alternative approach\n",
    "        try:\n",
    "            quick_ensemble = quick_ensemble_with_existing_model()\n",
    "            if quick_ensemble:\n",
    "                # Test the quick ensemble\n",
    "                df = datasets_info['borderline']['dataframe']\n",
    "                X = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                if 'Label' in X:\n",
    "                    X.remove('Label')\n",
    "                \n",
    "                features = df[X].values\n",
    "                labels = df['Label'].values\n",
    "                \n",
    "                le = LabelEncoder()\n",
    "                labels_encoded = le.fit_transform(labels)\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                features_scaled = scaler.fit_transform(features)\n",
    "                \n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    features_scaled, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded\n",
    "                )\n",
    "                \n",
    "                preds = quick_ensemble.predict(X_test)\n",
    "                acc = accuracy_score(y_test, preds)\n",
    "                \n",
    "                print(f\"ğŸ¯ Quick Ensemble Accuracy: {acc:.4f}\")\n",
    "                \n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Alternative approach also failed: {e2}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
