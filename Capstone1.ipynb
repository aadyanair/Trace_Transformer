{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a2834d5-faec-49b3-b108-a342abd69ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (5138529, 80)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\03-01-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-28-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-21-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-16-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\03-02-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-14-2018.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(fp,low_memory=False) for fp in file_paths]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"Combined shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f69b7d-ace7-4e01-b6ee-c1e235fd2f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2960c198-461e-4035-a882-022aa72083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=100_000, random_state=42).copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Extract and encode the target labels\n",
    "y_raw = df_sample['Label']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51d4a982-ea20-4b1d-9d64-65e0dd3be64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned shape: (100000, 78)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numeric\n",
    "df_converted = df_sample.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "threshold = 0.5\n",
    "df_reduced = df_converted.loc[:, df_converted.isnull().mean() < threshold]\n",
    "\n",
    "# Fill remaining NaNs with column means\n",
    "df_cleaned = df_reduced.copy()\n",
    "for col in df_cleaned.columns:\n",
    "    if df_cleaned[col].isnull().any():\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n",
    "\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58ddc11b-49ba-4219-87dd-c8c3b710ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs: 0\n",
      "Infs: 652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaNs\n",
    "print(\"NaNs:\", df_cleaned.isna().sum().sum())\n",
    "\n",
    "# Check for Infs\n",
    "print(\"Infs:\",np.isinf(df_cleaned.values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad734e2-6c36-4477-8c76-24537f1e0117",
   "metadata": {},
   "source": [
    "Since the count is zero we need to clean them up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef8a27-dd8f-405e-a42e-7cf26e1fce3a",
   "metadata": {},
   "source": [
    "##### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d663433d-a277-4089-9a1a-9af126929d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inf/-inf with NaN\n",
    "df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill remaining NaNs with column means\n",
    "df_cleaned = df_cleaned.apply(lambda col: col.fillna(col.mean()) if col.dtype != 'object' else col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c216099f-f6c3-48e1-bbc6-41873c5707bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cleaned.select_dtypes(include=[np.number]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae157c22-4ffc-4aed-9a7e-7c5de60c1e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ca52e-5209-4af8-a5db-474dd48f28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c79a97-30a7-4bbc-8ccd-0562bdb450cd",
   "metadata": {},
   "source": [
    "### Interaction Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97bf2eda-fb1b-43ea-8136-3303f1780324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.4558\n",
      "Epoch 2/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1501\n",
      "Epoch 3/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0825\n",
      "Epoch 4/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0465\n",
      "Epoch 5/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0335\n",
      "Epoch 6/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0265\n",
      "Epoch 7/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0241\n",
      "Epoch 8/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0220\n",
      "Epoch 9/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0215\n",
      "Epoch 10/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0184\n",
      "Epoch 11/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0169\n",
      "Epoch 12/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0216\n",
      "Epoch 13/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0166\n",
      "Epoch 14/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0123\n",
      "Epoch 15/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0158\n",
      "Epoch 16/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0129\n",
      "Epoch 17/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0108\n",
      "Epoch 18/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0107\n",
      "Epoch 19/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0088\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 32  # You can tune this\n",
    "\n",
    "# Define layers\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Build model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Compile and train\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=20, batch_size=256, shuffle=True)\n",
    "\n",
    "# Get compressed features\n",
    "X_encoded = encoder.predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fc4052f-d6a7-4f1d-b586-d3ec6be1eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_15528\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 137 interaction features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_cleaned[numeric_cols].corr().abs()\n",
    "\n",
    "# Get upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find pairs with correlation > 0.7 (tune this threshold as needed)\n",
    "high_corr_pairs = [(col1, col2) for col1 in upper_tri.columns for col2 in upper_tri.index if upper_tri.loc[col2, col1] > 0.7]\n",
    "\n",
    "# Create interaction features\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    new_col_name = f\"{col1}_x_{col2}\"\n",
    "    df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
    "    \n",
    "df_cleaned = df_cleaned.copy()\n",
    "\n",
    "print(f\"✅ Created {len(high_corr_pairs)} interaction features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db1adb19-797b-448d-a148-bdf0057720d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066dac1-04aa-40f9-87c8-e4b5143c6129",
   "metadata": {},
   "source": [
    "### Train and Extract Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddd6acbf-f53c-4392-bfe7-07e7fc4bc03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.2197\n",
      "Epoch 2/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0515\n",
      "Epoch 3/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0325\n",
      "Epoch 4/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0400\n",
      "Epoch 5/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0565\n",
      "Epoch 6/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0442\n",
      "Epoch 7/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0415\n",
      "Epoch 8/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0457\n",
      "Epoch 9/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0467\n",
      "Epoch 10/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0238\n",
      "Epoch 11/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0470\n",
      "Epoch 12/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0623\n",
      "Epoch 13/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0523\n",
      "Epoch 14/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0377\n",
      "Epoch 15/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0361\n",
      "Epoch 16/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0225\n",
      "Epoch 17/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0262\n",
      "Epoch 18/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0351\n",
      "Epoch 19/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0585\n",
      "Epoch 20/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0299\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step\n",
      "✅ Added 32 autoencoder embeddings to your dataset.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define input dimension\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 32  # Bottleneck size\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "bottleneck = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation='relu')(bottleneck)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "# Build models\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "# Compile and train\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=20, batch_size=256, shuffle=True, verbose=1)\n",
    "\n",
    "# Extract embeddings\n",
    "X_embeddings = encoder.predict(X_scaled)\n",
    "\n",
    "# Add embeddings to DataFrame\n",
    "for i in range(encoding_dim):\n",
    "    df_cleaned[f'embed_{i}'] = X_embeddings[:, i]\n",
    "\n",
    "print(f\"✅ Added {encoding_dim} autoencoder embeddings to your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27c7fd-6bdf-458d-8c1f-02f25a248999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d1563-be8c-4b3f-bda2-4cf8e959fc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
