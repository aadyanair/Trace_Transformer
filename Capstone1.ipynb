{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2834d5-faec-49b3-b108-a342abd69ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (5138529, 80)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\03-01-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-28-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-21-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-16-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\03-02-2018.csv\",\n",
    "    r\"C:\\Users\\Aadya Nair\\OneDrive\\Documents\\Projects\\Dataset\\02-14-2018.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(fp,low_memory=False) for fp in file_paths]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"Combined shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f69b7d-ace7-4e01-b6ee-c1e235fd2f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2960c198-461e-4035-a882-022aa72083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=100_000, random_state=42).copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Extract and encode the target labels\n",
    "y_raw = df_sample['Label']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d4a982-ea20-4b1d-9d64-65e0dd3be64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned shape: (100000, 78)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numeric\n",
    "df_converted = df_sample.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop columns with >50% missing values\n",
    "threshold = 0.5\n",
    "df_reduced = df_converted.loc[:, df_converted.isnull().mean() < threshold]\n",
    "\n",
    "# Fill remaining NaNs with column means\n",
    "df_cleaned = df_reduced.copy()\n",
    "for col in df_cleaned.columns:\n",
    "    if df_cleaned[col].isnull().any():\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n",
    "\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ddc11b-49ba-4219-87dd-c8c3b710ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs: 0\n",
      "Infs: 652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaNs\n",
    "print(\"NaNs:\", df_cleaned.isna().sum().sum())\n",
    "\n",
    "# Check for Infs\n",
    "print(\"Infs:\",np.isinf(df_cleaned.values).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad734e2-6c36-4477-8c76-24537f1e0117",
   "metadata": {},
   "source": [
    "Since the count is zero we need to clean them up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef8a27-dd8f-405e-a42e-7cf26e1fce3a",
   "metadata": {},
   "source": [
    "##### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d663433d-a277-4089-9a1a-9af126929d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inf/-inf with NaN\n",
    "df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill remaining NaNs with column means\n",
    "df_cleaned = df_cleaned.apply(lambda col: col.fillna(col.mean()) if col.dtype != 'object' else col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c216099f-f6c3-48e1-bbc6-41873c5707bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cleaned.select_dtypes(include=[np.number]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae157c22-4ffc-4aed-9a7e-7c5de60c1e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ca52e-5209-4af8-a5db-474dd48f28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c79a97-30a7-4bbc-8ccd-0562bdb450cd",
   "metadata": {},
   "source": [
    "### Interaction Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bf2eda-fb1b-43ea-8136-3303f1780324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.4201\n",
      "Epoch 2/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1395\n",
      "Epoch 3/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0771\n",
      "Epoch 4/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0486\n",
      "Epoch 5/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0378\n",
      "Epoch 6/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0291\n",
      "Epoch 7/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0244\n",
      "Epoch 8/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0237\n",
      "Epoch 9/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0193\n",
      "Epoch 10/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0162\n",
      "Epoch 11/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0143\n",
      "Epoch 12/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0157\n",
      "Epoch 13/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0144\n",
      "Epoch 14/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0178\n",
      "Epoch 15/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0121\n",
      "Epoch 16/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0149\n",
      "Epoch 17/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0143\n",
      "Epoch 18/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0102\n",
      "Epoch 20/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0121\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 32  # You can tune this\n",
    "\n",
    "# Define layers\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "# Build model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Compile and train\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=20, batch_size=256, shuffle=True)\n",
    "\n",
    "# Get compressed features\n",
    "X_encoded = encoder.predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc4052f-d6a7-4f1d-b586-d3ec6be1eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
      "C:\\Users\\Aadya Nair\\AppData\\Local\\Temp\\ipykernel_21940\\958458506.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 137 interaction features.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_cleaned[numeric_cols].corr().abs()\n",
    "\n",
    "# Get upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find pairs with correlation > 0.7 (tune this threshold as needed)\n",
    "high_corr_pairs = [(col1, col2) for col1 in upper_tri.columns for col2 in upper_tri.index if upper_tri.loc[col2, col1] > 0.7]\n",
    "\n",
    "# Create interaction features\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    new_col_name = f\"{col1}_x_{col2}\"\n",
    "    df_cleaned[new_col_name] = df_cleaned[col1] * df_cleaned[col2]\n",
    "    \n",
    "df_cleaned = df_cleaned.copy()\n",
    "\n",
    "print(f\"✅ Created {len(high_corr_pairs)} interaction features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1adb19-797b-448d-a148-bdf0057720d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066dac1-04aa-40f9-87c8-e4b5143c6129",
   "metadata": {},
   "source": [
    "### Train and Extract Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd6acbf-f53c-4392-bfe7-07e7fc4bc03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.2446\n",
      "Epoch 2/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0709\n",
      "Epoch 3/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0503\n",
      "Epoch 4/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0870\n",
      "Epoch 5/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0306\n",
      "Epoch 6/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0382\n",
      "Epoch 7/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0684\n",
      "Epoch 8/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0417\n",
      "Epoch 9/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0469\n",
      "Epoch 10/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0459\n",
      "Epoch 11/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0416\n",
      "Epoch 12/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0374\n",
      "Epoch 13/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0260\n",
      "Epoch 14/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0322\n",
      "Epoch 15/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0353\n",
      "Epoch 16/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0384\n",
      "Epoch 17/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0321\n",
      "Epoch 18/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0363\n",
      "Epoch 19/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0299\n",
      "Epoch 20/20\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0453\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
      "✅ Added 32 autoencoder embeddings to your dataset.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define input dimension\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 32  # Bottleneck size\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "bottleneck = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(64, activation='relu')(bottleneck)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "# Build models\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "# Compile and train\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=20, batch_size=256, shuffle=True, verbose=1)\n",
    "\n",
    "# Extract embeddings\n",
    "X_embeddings = encoder.predict(X_scaled)\n",
    "\n",
    "# Add embeddings to DataFrame\n",
    "for i in range(encoding_dim):\n",
    "    df_cleaned[f'embed_{i}'] = X_embeddings[:, i]\n",
    "\n",
    "print(f\"✅ Added {encoding_dim} autoencoder embeddings to your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27c7fd-6bdf-458d-8c1f-02f25a248999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a6bf67-9e99-4e91-98b5-beef82c19490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\aadya nair\\appdata\\roaming\\python\\python313\\site-packages (from imbalanced-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\python313\\lib\\site-packages (from imbalanced-learn) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\python313\\lib\\site-packages (from imbalanced-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\python313\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\python313\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Downloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d1563-be8c-4b3f-bda2-4cf8e959fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Final Feature Matrix\n",
    "X_final = df_cleaned.copy()\n",
    "y_final = df_sample['Label'] \n",
    "assert len(X_final) == len(y_final), \"Mismatch in feature and label lengths!\"\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_final_encoded = le.fit_transform(y_final)\n",
    "\n",
    "# 2. Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final_encoded, test_size=0.2, stratify=y_final_encoded, random_state = 42)\n",
    "\n",
    "# 3. Class Balancing with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 4. Define Models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"HGB\": HistGradientBoostingClassifier(),\n",
    "    \"TabNet\": TabNetClassifier(verbose=0, device_name='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "# 5. Train and Evaluate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n 🔍 Training {name}...\")\n",
    "\n",
    "    if name == \"TabNet\":\n",
    "        model.fit(X_train_balanced.values, y_train_balanced, eval_set=[(X_test.values, y_test)], patience=10)\n",
    "        y_pred = model.predict(X_test.values)\n",
    "        y_proba = model.predict_proba(X_test.values)\n",
    "    else:\n",
    "        model.fit(X_train_balanced, y_train_balanced)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "\n",
    "        # Pad y_proba if some classes are missing\n",
    "        expected_classes = np.unique(y_train_balanced)\n",
    "        if y_proba.shape[1] != len(expected_classes):\n",
    "            missing = len(expected_classes) - y_proba.shape[1]\n",
    "            pad = np.zeros((y_proba.shape[0], missing))\n",
    "            y_proba = np.hstack([y_proba, pad])\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    roc = roc_auc_score(label_binarize(y_test, classes=np.unique(y_test)), y_proba, average='weighted', multi_class='ovr')\n",
    "\n",
    "    results[name] = {\"Accuracy\": acc, \"F1 Score\": f1, \"ROC-AUC\": roc}\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# 6. Display Results\n",
    "for model_name, scores in results.items():\n",
    "    print(f\"\\n 📊 {model_name} Results:\")\n",
    "    for metric, value in scores.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
