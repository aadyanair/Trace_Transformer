{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b2346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (0.49.1)\n",
      "Requirement already satisfied: lime in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: torch in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (1.16.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from lime) (0.25.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from plotly) (2.10.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from scikit-image>=0.12->lime) (2025.10.16)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shraddha\\onedrive\\desktop\\tt\\trace_transformer\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install shap lime torch pandas numpy scikit-learn matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee87dac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shraddha\\OneDrive\\Desktop\\TT\\Trace_Transformer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eebbe5",
   "metadata": {},
   "source": [
    "Load the data files in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "# Note: Make sure you have both balanced dataset files in your workspace\n",
    "df_borderline = pd.read_csv('balanced_dataset_borderline_smote.csv')\n",
    "df_adasyn = pd.read_csv('balanced_dataset_adasyn.csv')\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(df, test_size=0.2, random_state=42):\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values)\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'X_train_scaled': X_train_scaled, 'X_test_scaled': X_test_scaled,\n",
    "        'X_train_tensor': X_train_tensor, 'X_test_tensor': X_test_tensor,\n",
    "        'y_train_tensor': y_train_tensor, 'y_test_tensor': y_test_tensor,\n",
    "        'scaler': scaler, 'feature_names': X.columns\n",
    "    }\n",
    "\n",
    "# Prepare both datasets\n",
    "data_borderline = prepare_data(df_borderline)\n",
    "data_adasyn = prepare_data(df_adasyn)\n",
    "\n",
    "print(\"âœ… Data loaded and prepared successfully!\")\n",
    "print(\"\\nBorderline-SMOTE Dataset:\")\n",
    "print(f\"Training set shape: {data_borderline['X_train_scaled'].shape}\")\n",
    "print(f\"Test set shape: {data_borderline['X_test_scaled'].shape}\")\n",
    "print(\"\\nADASYN Dataset:\")\n",
    "print(f\"Training set shape: {data_adasyn['X_train_scaled'].shape}\")\n",
    "print(f\"Test set shape: {data_adasyn['X_test_scaled'].shape}\")\n",
    "\n",
    "# We'll use these variables for compatibility with existing code\n",
    "# Default to Borderline-SMOTE data\n",
    "X_train = data_borderline['X_train']\n",
    "X_test = data_borderline['X_test']\n",
    "y_train = data_borderline['y_train']\n",
    "y_test = data_borderline['y_test']\n",
    "X_train_scaled = data_borderline['X_train_scaled']\n",
    "X_test_scaled = data_borderline['X_test_scaled']\n",
    "X_train_tensor = data_borderline['X_train_tensor']\n",
    "X_test_tensor = data_borderline['X_test_tensor']\n",
    "y_train_tensor = data_borderline['y_train_tensor']\n",
    "y_test_tensor = data_borderline['y_test_tensor']\n",
    "scaler = data_borderline['scaler']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96622f9",
   "metadata": {},
   "source": [
    "# Interpretability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up interpretability for FTTransformer\n",
    "class InterpretableFTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dim=128, depth=6, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.feature_embedding = nn.Linear(num_features, dim)\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=dim,\n",
    "                nhead=heads,\n",
    "                dim_feedforward=dim*4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        self.attention_weights = []  # Reset attention weights\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        x = self.feature_embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x + self.pos_encoding\n",
    "        \n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            # Store attention weights for visualization\n",
    "            if return_attention:\n",
    "                self.attention_weights.append(\n",
    "                    transformer_layer.self_attn(x, x, x)[1]\n",
    "                )\n",
    "            x = transformer_layer(x)\n",
    "        \n",
    "        x = x.mean(dim=1)\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, self.attention_weights\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InterpretableFTTransformer(\n",
    "    num_features=X_train.shape[1],\n",
    "    num_classes=len(np.unique(y_train)),\n",
    ").to(device)\n",
    "\n",
    "print(\"âœ… Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e71858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor.to(device))\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    correct = (predicted == y_test_tensor.to(device)).sum().item()\n",
    "    accuracy = correct / len(y_test_tensor)\n",
    "\n",
    "print(f\"\\nâœ… Model training completed!\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810457ee",
   "metadata": {},
   "source": [
    "# SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP (SHapley Additive exPlanations) analysis to understand feature importance and model decisions.\n",
    "class ShapExplainer:\n",
    "    def __init__(self, model, feature_names):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def explain_instance(self, instance, background_data):\n",
    "        # Create a PyTorch model wrapper for SHAP\n",
    "        def model_predict(x):\n",
    "            x_tensor = torch.FloatTensor(x).to(device)\n",
    "            with torch.no_grad():\n",
    "                return self.model(x_tensor).cpu().numpy()\n",
    "        \n",
    "        # Initialize the SHAP explainer\n",
    "        explainer = shap.KernelExplainer(model_predict, background_data)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(instance)\n",
    "        \n",
    "        return shap_values\n",
    "    \n",
    "    def plot_feature_importance(self, instance, background_data):\n",
    "        shap_values = self.explain_instance(instance, background_data)\n",
    "        \n",
    "        # Create summary plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.summary_plot(\n",
    "            shap_values[1] if isinstance(shap_values, list) else shap_values,\n",
    "            instance,\n",
    "            feature_names=self.feature_names,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(\"SHAP Feature Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = ShapExplainer(model, X_train.columns)\n",
    "\n",
    "# Get background data for SHAP (using a subset of training data)\n",
    "background_data = X_train_scaled[:100]  # Using first 100 samples as background\n",
    "\n",
    "# Example: Explain a single instance\n",
    "instance_to_explain = X_test_scaled[0:1]  # First test instance\n",
    "explainer.plot_feature_importance(instance_to_explain, background_data)\n",
    "\n",
    "print(\"âœ… SHAP analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d28281",
   "metadata": {},
   "source": [
    "# Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4550e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the attention weights from the transformer model to understand which features the model focuses on.\n",
    "class AttentionVisualizer:\n",
    "    def __init__(self, model, feature_names):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def get_attention_weights(self, input_data):\n",
    "        input_tensor = torch.FloatTensor(input_data).to(device)\n",
    "        with torch.no_grad():\n",
    "            _, attention_weights = self.model(input_tensor, return_attention=True)\n",
    "        return attention_weights\n",
    "    \n",
    "    def plot_attention_heatmap(self, input_data, layer_idx=0):\n",
    "        attention_weights = self.get_attention_weights(input_data)\n",
    "        \n",
    "        # Get attention weights for specified layer\n",
    "        layer_attention = attention_weights[layer_idx][0].cpu().numpy()\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(\n",
    "            layer_attention,\n",
    "            xticklabels=self.feature_names,\n",
    "            yticklabels=self.feature_names,\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        plt.title(f'Attention Weights - Layer {layer_idx + 1}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_attention_summary(self, input_data):\n",
    "        attention_weights = self.get_attention_weights(input_data)\n",
    "        \n",
    "        # Average attention across all layers\n",
    "        avg_attention = torch.mean(\n",
    "            torch.stack([w[0] for w in attention_weights]), \n",
    "            dim=0\n",
    "        ).cpu().numpy()\n",
    "        \n",
    "        # Calculate mean attention per feature\n",
    "        feature_attention = avg_attention.mean(axis=0)\n",
    "        \n",
    "        # Create bar plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(self.feature_names, feature_attention)\n",
    "        plt.title('Average Feature Attention')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = AttentionVisualizer(model, X_train.columns)\n",
    "\n",
    "# Visualize attention for a single instance\n",
    "instance_to_visualize = X_test_scaled[0:1]\n",
    "visualizer.plot_attention_heatmap(instance_to_visualize)\n",
    "visualizer.plot_feature_attention_summary(instance_to_visualize)\n",
    "\n",
    "print(\"âœ… Attention visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d78959",
   "metadata": {},
   "source": [
    "# Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate counterfactual explanations to show how predictions would change with different inputs.\n",
    "class CounterfactualGenerator:\n",
    "    def __init__(self, model, feature_names, scaler):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def generate_counterfactuals(self, instance, target_class, num_samples=1000, eps=0.1):\n",
    "        # Generate random perturbations\n",
    "        noise = np.random.normal(0, eps, size=(num_samples, instance.shape[1]))\n",
    "        candidates = instance + noise\n",
    "        \n",
    "        # Get predictions for all candidates\n",
    "        candidates_tensor = torch.FloatTensor(candidates).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(candidates_tensor)\n",
    "            probs = torch.softmax(predictions, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Find the closest counterfactual\n",
    "        target_probs = probs[:, target_class]\n",
    "        best_idx = np.argmax(target_probs)\n",
    "        \n",
    "        return candidates[best_idx], target_probs[best_idx]\n",
    "    \n",
    "    def explain_counterfactual(self, original_instance, counterfactual):\n",
    "        # Calculate and sort feature differences\n",
    "        diff = counterfactual - original_instance\n",
    "        feature_diffs = list(zip(self.feature_names, diff[0]))\n",
    "        feature_diffs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        features, values = zip(*[(f, v) for f, v in feature_diffs if abs(v) > 0.01])\n",
    "        \n",
    "        plt.bar(features, values)\n",
    "        plt.title('Feature Changes for Counterfactual Example')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Change in Feature Value')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_diffs\n",
    "\n",
    "# Initialize counterfactual generator\n",
    "counterfactual_gen = CounterfactualGenerator(model, X_train.columns, scaler)\n",
    "\n",
    "# Generate counterfactual for a test instance\n",
    "instance_to_explain = X_test_scaled[0:1]\n",
    "target_class = 1  # Desired target class\n",
    "counterfactual, confidence = counterfactual_gen.generate_counterfactuals(\n",
    "    instance_to_explain, target_class\n",
    ")\n",
    "\n",
    "# Explain the counterfactual\n",
    "feature_differences = counterfactual_gen.explain_counterfactual(\n",
    "    instance_to_explain, counterfactual\n",
    ")\n",
    "\n",
    "print(\"âœ… Counterfactual explanation generated!\")\n",
    "print(f\"Confidence in target class: {confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f74ba0",
   "metadata": {},
   "source": [
    "# Zero-Trust Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e59692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement zero-trust security measures for the ML pipeline.\n",
    "class ZeroTrustWrapper:\n",
    "    def __init__(self, model, feature_names, scaler):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.scaler = scaler\n",
    "        self.feature_stats = self._compute_feature_stats()\n",
    "    \n",
    "    def _compute_feature_stats(self):\n",
    "        \"\"\"Compute statistics for input validation\"\"\"\n",
    "        return {\n",
    "            'mean': np.mean(X_train, axis=0),\n",
    "            'std': np.std(X_train, axis=0),\n",
    "            'min': np.min(X_train, axis=0),\n",
    "            'max': np.max(X_train, axis=0)\n",
    "        }\n",
    "    \n",
    "    def _validate_input(self, X):\n",
    "        \"\"\"Validate input data\"\"\"\n",
    "        validations = []\n",
    "        \n",
    "        # Check for missing values\n",
    "        if np.any(np.isnan(X)):\n",
    "            validations.append(\"Input contains missing values\")\n",
    "        \n",
    "        # Check for infinity values\n",
    "        if np.any(np.isinf(X)):\n",
    "            validations.append(\"Input contains infinity values\")\n",
    "        \n",
    "        # Check for out-of-bounds values (using z-score)\n",
    "        z_scores = np.abs((X - self.feature_stats['mean']) / self.feature_stats['std'])\n",
    "        if np.any(z_scores > 5):  # More than 5 standard deviations\n",
    "            validations.append(\"Input contains extreme values\")\n",
    "        \n",
    "        return validations\n",
    "    \n",
    "    def _check_model_confidence(self, probabilities, threshold=0.8):\n",
    "        \"\"\"Check if model confidence is above threshold\"\"\"\n",
    "        max_prob = np.max(probabilities)\n",
    "        return max_prob >= threshold, max_prob\n",
    "    \n",
    "    def predict(self, X, return_diagnostics=False):\n",
    "        # Input validation\n",
    "        validation_results = self._validate_input(X)\n",
    "        \n",
    "        # Transform input\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        \n",
    "        # Model prediction with confidence check\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        confident, confidence = self._check_model_confidence(probabilities)\n",
    "        predictions = np.argmax(probabilities, axis=1)\n",
    "        \n",
    "        if return_diagnostics:\n",
    "            diagnostics = {\n",
    "                'validation_results': validation_results,\n",
    "                'confidence': confidence,\n",
    "                'is_confident': confident,\n",
    "                'probabilities': probabilities\n",
    "            }\n",
    "            return predictions, diagnostics\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Initialize zero-trust wrapper\n",
    "zero_trust_model = ZeroTrustWrapper(model, X_train.columns, scaler)\n",
    "\n",
    "# Test prediction with diagnostics\n",
    "test_instance = X_test[:1]\n",
    "predictions, diagnostics = zero_trust_model.predict(test_instance, return_diagnostics=True)\n",
    "\n",
    "print(\"âœ… Zero-trust prediction completed!\")\n",
    "print(\"\\nDiagnostics:\")\n",
    "print(f\"Validation results: {diagnostics['validation_results'] if diagnostics['validation_results'] else 'All validations passed'}\")\n",
    "print(f\"Confidence: {diagnostics['confidence']:.3f}\")\n",
    "print(f\"Is confident: {diagnostics['is_confident']}\")\n",
    "print(f\"Prediction: {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad395269",
   "metadata": {},
   "source": [
    "# Explainable AI Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainableAIWrapper:\n",
    "    def __init__(self, model, data_borderline, data_adasyn):\n",
    "        self.model = model\n",
    "        self.data_borderline = data_borderline\n",
    "        self.data_adasyn = data_adasyn\n",
    "        \n",
    "        # Initialize components for both datasets\n",
    "        self.components = {\n",
    "            'borderline': {\n",
    "                'zero_trust': ZeroTrustWrapper(model, data_borderline['feature_names'], data_borderline['scaler']),\n",
    "                'shap_explainer': ShapExplainer(model, data_borderline['feature_names']),\n",
    "                'attention_viz': AttentionVisualizer(model, data_borderline['feature_names']),\n",
    "                'counterfactual_gen': CounterfactualGenerator(model, data_borderline['feature_names'], data_borderline['scaler'])\n",
    "            },\n",
    "            'adasyn': {\n",
    "                'zero_trust': ZeroTrustWrapper(model, data_adasyn['feature_names'], data_adasyn['scaler']),\n",
    "                'shap_explainer': ShapExplainer(model, data_adasyn['feature_names']),\n",
    "                'attention_viz': AttentionVisualizer(model, data_adasyn['feature_names']),\n",
    "                'counterfactual_gen': CounterfactualGenerator(model, data_adasyn['feature_names'], data_adasyn['scaler'])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def explain_prediction(self, instance, dataset_type='borderline', background_data=None):\n",
    "        \"\"\"Generate a comprehensive explanation for a prediction\"\"\"\n",
    "        components = self.components[dataset_type]\n",
    "        data = self.data_borderline if dataset_type == 'borderline' else self.data_adasyn\n",
    "        \n",
    "        print(f\"\\n{'='*20} Analysis using {dataset_type.upper()} Dataset {'='*20}\")\n",
    "        \n",
    "        # Get prediction with zero-trust\n",
    "        pred, diagnostics = components['zero_trust'].predict(instance, return_diagnostics=True)\n",
    "        \n",
    "        print(\"\\n1. Zero-Trust Diagnostics:\")\n",
    "        print(f\"Prediction: {pred[0]}\")\n",
    "        print(f\"Confidence: {diagnostics['confidence']:.3f}\")\n",
    "        print(f\"Validation Results: {diagnostics['validation_results'] if diagnostics['validation_results'] else 'All validations passed'}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Get SHAP explanation\n",
    "        print(\"2. SHAP Feature Importance:\")\n",
    "        if background_data is None:\n",
    "            background_data = data['X_train_scaled'][:100]\n",
    "        components['shap_explainer'].plot_feature_importance(\n",
    "            data['scaler'].transform(instance), \n",
    "            background_data\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Get attention visualization\n",
    "        print(\"3. Attention Analysis:\")\n",
    "        components['attention_viz'].plot_attention_heatmap(\n",
    "            data['scaler'].transform(instance)\n",
    "        )\n",
    "        components['attention_viz'].plot_feature_attention_summary(\n",
    "            data['scaler'].transform(instance)\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Generate counterfactual\n",
    "        print(\"4. Counterfactual Explanation:\")\n",
    "        target_class = 1 if pred[0] == 0 else 0  # Opposite class\n",
    "        counterfactual, confidence = components['counterfactual_gen'].generate_counterfactuals(\n",
    "            data['scaler'].transform(instance),\n",
    "            target_class\n",
    "        )\n",
    "        components['counterfactual_gen'].explain_counterfactual(\n",
    "            data['scaler'].transform(instance),\n",
    "            counterfactual\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'prediction': pred[0],\n",
    "            'diagnostics': diagnostics,\n",
    "            'counterfactual_confidence': confidence\n",
    "        }\n",
    "    \n",
    "    def compare_explanations(self, instance):\n",
    "        \"\"\"Compare explanations between Borderline-SMOTE and ADASYN datasets\"\"\"\n",
    "        borderline_results = self.explain_prediction(instance, 'borderline')\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\nComparing with ADASYN dataset:\\n\" + \"=\"*50 + \"\\n\")\n",
    "        adasyn_results = self.explain_prediction(instance, 'adasyn')\n",
    "        \n",
    "        # Compare predictions and confidence\n",
    "        print(\"\\nðŸ“Š Comparison Summary:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Predictions:\")\n",
    "        print(f\"â€¢ Borderline-SMOTE: {borderline_results['prediction']} (conf: {borderline_results['diagnostics']['confidence']:.3f})\")\n",
    "        print(f\"â€¢ ADASYN: {adasyn_results['prediction']} (conf: {adasyn_results['diagnostics']['confidence']:.3f})\")\n",
    "        \n",
    "        # Compare counterfactual confidence\n",
    "        print(\"\\nCounterfactual Generation:\")\n",
    "        print(f\"â€¢ Borderline-SMOTE confidence: {borderline_results['counterfactual_confidence']:.3f}\")\n",
    "        print(f\"â€¢ ADASYN confidence: {adasyn_results['counterfactual_confidence']:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'borderline': borderline_results,\n",
    "            'adasyn': adasyn_results\n",
    "        }\n",
    "\n",
    "# Initialize explainable AI wrapper with both datasets\n",
    "xai_wrapper = ExplainableAIWrapper(model, data_borderline, data_adasyn)\n",
    "\n",
    "# Generate comprehensive explanation for a test instance using both datasets\n",
    "test_instance = X_test[:1]\n",
    "comparison_results = xai_wrapper.compare_explanations(test_instance)\n",
    "\n",
    "print(\"\\nâœ… Comparative analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f55f7d",
   "metadata": {},
   "source": [
    "# Online Learning Integration\n",
    "\n",
    "Implement online learning capabilities to update the model with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLearner:\n",
    "    def __init__(self, model, scaler, buffer_size=1000):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer_X = []\n",
    "        self.buffer_y = []\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def update(self, X, y):\n",
    "        \"\"\"Update the model with new data\"\"\"\n",
    "        # Add to buffer\n",
    "        self.buffer_X.extend(X)\n",
    "        self.buffer_y.extend(y)\n",
    "        \n",
    "        # Maintain buffer size\n",
    "        if len(self.buffer_X) > self.buffer_size:\n",
    "            self.buffer_X = self.buffer_X[-self.buffer_size:]\n",
    "            self.buffer_y = self.buffer_y[-self.buffer_size:]\n",
    "        \n",
    "        # Prepare data\n",
    "        X_scaled = self.scaler.transform(np.array(self.buffer_X))\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        y_tensor = torch.LongTensor(self.buffer_y).to(device)\n",
    "        \n",
    "        # Update model\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(X_tensor)\n",
    "        loss = self.criterion(outputs, y_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def evaluate_performance(self, X_test, y_test):\n",
    "        \"\"\"Evaluate current model performance\"\"\"\n",
    "        self.model.eval()\n",
    "        X_scaled = self.scaler.transform(X_test)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        predicted = predicted.cpu().numpy()\n",
    "        return classification_report(y_test, predicted)\n",
    "\n",
    "# Initialize online learner\n",
    "online_learner = OnlineLearner(model, scaler)\n",
    "\n",
    "# Simulate online learning with new data\n",
    "# For demonstration, we'll use some test data as new data\n",
    "new_data_X = X_test[:10]\n",
    "new_data_y = y_test[:10]\n",
    "\n",
    "# Update model\n",
    "loss = online_learner.update(new_data_X, new_data_y)\n",
    "\n",
    "# Evaluate updated model\n",
    "performance_report = online_learner.evaluate_performance(X_test, y_test)\n",
    "\n",
    "print(\"âœ… Online learning update completed!\")\n",
    "print(f\"Update loss: {loss:.4f}\\n\")\n",
    "print(\"Updated Model Performance:\")\n",
    "print(performance_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caa5aa",
   "metadata": {},
   "source": [
    "# Project Summary and Conclusions\n",
    "\n",
    "This section provides a comprehensive summary of the Trace Transformer project's interpretability and innovation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52756fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive project summary comparing both approaches\n",
    "def generate_project_summary(model, data_borderline, data_adasyn):\n",
    "    summary = {\n",
    "        'model_performance': {\n",
    "            'borderline': {},\n",
    "            'adasyn': {}\n",
    "        },\n",
    "        'interpretability_metrics': {\n",
    "            'borderline': {},\n",
    "            'adasyn': {}\n",
    "        },\n",
    "        'comparative_analysis': {}\n",
    "    }\n",
    "    \n",
    "    # Function to evaluate model performance\n",
    "    def evaluate_dataset(data, dataset_name):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_tensor = torch.FloatTensor(data['X_test_scaled']).to(device)\n",
    "            outputs = model(test_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        predictions = predicted.cpu().numpy()\n",
    "        test_labels = data['y_test'].values\n",
    "        confidence_scores = np.max(probabilities, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (predictions == test_labels).mean()\n",
    "        avg_confidence = confidence_scores.mean()\n",
    "        high_conf_preds = (confidence_scores > 0.9).mean()\n",
    "        \n",
    "        # Get feature importance from attention weights\n",
    "        visualizer = AttentionVisualizer(model, data['feature_names'])\n",
    "        sample_instance = data['X_test_scaled'][0:1]\n",
    "        attention_weights = visualizer.get_attention_weights(sample_instance)\n",
    "        avg_attention = torch.mean(torch.stack([w[0] for w in attention_weights]), dim=0).cpu().numpy()\n",
    "        feature_importance = avg_attention.mean(axis=0)\n",
    "        \n",
    "        # Get top important features\n",
    "        feature_importance_dict = dict(zip(data['feature_names'], feature_importance))\n",
    "        top_features = dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'average_confidence': avg_confidence,\n",
    "            'high_confidence_predictions': high_conf_preds,\n",
    "            'top_features': top_features,\n",
    "            'attention_stability': np.std(feature_importance)\n",
    "        }\n",
    "    \n",
    "    # Evaluate both datasets\n",
    "    borderline_metrics = evaluate_dataset(data_borderline, 'Borderline-SMOTE')\n",
    "    adasyn_metrics = evaluate_dataset(data_adasyn, 'ADASYN')\n",
    "    \n",
    "    # Store results\n",
    "    for dataset_name, metrics in [('borderline', borderline_metrics), ('adasyn', adasyn_metrics)]:\n",
    "        summary['model_performance'][dataset_name] = {\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'average_confidence': metrics['average_confidence'],\n",
    "            'high_confidence_predictions': metrics['high_confidence_predictions']\n",
    "        }\n",
    "        summary['interpretability_metrics'][dataset_name] = {\n",
    "            'top_important_features': metrics['top_features'],\n",
    "            'attention_pattern_stability': metrics['attention_stability']\n",
    "        }\n",
    "    \n",
    "    # Comparative analysis\n",
    "    summary['comparative_analysis'] = {\n",
    "        'accuracy_difference': abs(borderline_metrics['accuracy'] - adasyn_metrics['accuracy']),\n",
    "        'confidence_difference': abs(borderline_metrics['average_confidence'] - adasyn_metrics['average_confidence']),\n",
    "        'shared_top_features': len(\n",
    "            set(borderline_metrics['top_features'].keys()) & \n",
    "            set(adasyn_metrics['top_features'].keys())\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Create formatted report\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸ” TRACE TRANSFORMER PROJECT SUMMARY - COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for dataset in ['borderline', 'adasyn']:\n",
    "        print(f\"\\nðŸ“Š {dataset.upper()} Dataset Performance:\")\n",
    "        print(f\"   â€¢ Accuracy: {summary['model_performance'][dataset]['accuracy']:.4f}\")\n",
    "        print(f\"   â€¢ Average Confidence: {summary['model_performance'][dataset]['average_confidence']:.4f}\")\n",
    "        print(f\"   â€¢ High Confidence Predictions: {summary['model_performance'][dataset]['high_confidence_predictions']*100:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Top Important Features ({dataset.upper()}):\")\n",
    "        for feature, importance in summary['interpretability_metrics'][dataset]['top_important_features'].items():\n",
    "            print(f\"   â€¢ {feature}: {importance:.4f}\")\n",
    "    \n",
    "    print(\"\\n\udcc8 Comparative Analysis:\")\n",
    "    print(f\"   â€¢ Accuracy Difference: {summary['comparative_analysis']['accuracy_difference']:.4f}\")\n",
    "    print(f\"   â€¢ Confidence Difference: {summary['comparative_analysis']['confidence_difference']:.4f}\")\n",
    "    print(f\"   â€¢ Shared Top Features: {summary['comparative_analysis']['shared_top_features']}/5\")\n",
    "    \n",
    "    print(\"\\n\udd27 Innovation Components Status:\")\n",
    "    print(\"   â€¢ Zero-trust Implementation: Active for both datasets\")\n",
    "    print(\"   â€¢ Explainable AI Framework: Enhanced with comparative analysis\")\n",
    "    print(\"   â€¢ Online Learning: Adaptive to both sampling methods\")\n",
    "    print(\"   â€¢ Attention Visualization: Comparative feature importance analysis\")\n",
    "    \n",
    "    print(\"\\nðŸš€ Key Findings:\")\n",
    "    if summary['model_performance']['borderline']['accuracy'] > summary['model_performance']['adasyn']['accuracy']:\n",
    "        better_method = \"Borderline-SMOTE\"\n",
    "    else:\n",
    "        better_method = \"ADASYN\"\n",
    "    print(f\"   â€¢ {better_method} shows better overall performance\")\n",
    "    print(f\"   â€¢ {summary['comparative_analysis']['shared_top_features']} shared important features between methods\")\n",
    "    print(\"   â€¢ Both methods maintain robust interpretability\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate and display the comparative summary\n",
    "summary = generate_project_summary(model, data_borderline, data_adasyn)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Comparative Project Summary Generated Successfully!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
